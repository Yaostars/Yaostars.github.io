[{"content":"","date":null,"permalink":"/zh-cn/tags/life/","section":"Tags","summary":"","title":"Life"},{"content":"","date":null,"permalink":"/zh-cn/tags/","section":"Tags","summary":"","title":"Tags"},{"content":"","date":null,"permalink":"/zh-cn/","section":"Ystars","summary":"","title":"Ystars"},{"content":"\r这是本博客的生活板块！🚀它将用于分享我周围的一些有趣的事情。你可以在评论区尽情评论，或者如果你也有一些有趣的经历，欢迎你在评论区分享。⭐\r保持每天愉快心情！ 😄 #","date":null,"permalink":"/zh-cn/life/","section":"生活","summary":"","title":"生活"},{"content":"很久没有和父母一起远游了，这在我小的时候本是一件常有的事。然而随着时光消逝，这种旅行竟也变得越来越少，以至于再次旅行时竟会产生一些别扭的情绪。\n广州 #\r广州是我们这次去的第一个地方，因为我将就读的香港科技大学广州校区就在这里。哦对，忘记说了，这次旅行也是我父母顺便为我送行，顺道来看看我的新学校。这是我第二次来到这座城市，而我的父母都是第一次来这里。\n在经历了飞机晚点的一夜折磨后，我和父母精疲力尽地踏上了广州这片土地。在宾馆休整完毕后，我们租了一辆车，正式开始了为期2天的广深自驾游。\n说实话，走遍了祖国的多个城市后，这些城市对我并没有多少的吸引力。就拿广州而言，所吸引我的不过是那地道的“粤菜馆”以及一些民国时期的古老建筑。在打卡永利饭店以后，对粤菜的感觉却也不咸不淡了。然而和父母一起旅游，纵然多些口角，却也少些烦恼。总体而言，在广州的一天还是非常愉快的。\n深圳 #\r从南沙经虎门上深圳，放眼望去，我已迷失于那烟雨中耸立的楼宇大厦。汽车穿梭在钢铁丛林之中，道路两旁皆是不可知其高的摩天大楼。所谓“City”，所谓“摩登”，莫不如是。我故知如此城市，亦有阴暗如城中村般的存在，然作为一个游客，又有多少人会去专注于那些如同自己一样尘埃中的人们呢？在某种角度上来说，深圳是一座近乎完美的城市，完美得生人勿近。\n香港 #\r如果说深圳是一座新秀的完美青年，那么香港或许是一个不服老的中年人。期间的原因难以言说，但作为一个游客，两趟天星小轮就已经让我体会到本次旅行的最大乐趣了，又何必深究？\n快乐的旅途总是短暂的，或许每一个想要长途旅游的人所求的都是一点自由、一点新奇，又或是一点不一样的生活方式，来给这本就平淡无聊的生活增添一点别样的乐趣。\n文笔随意，仅供自娱。\n","date":"2024年8月24日","permalink":"/zh-cn/life/travel_in_guangdong/","section":"生活","summary":"一次值得追忆的旅程\u0026hellip;\u0026hellip;","title":"珠三角之行"},{"content":"一直喜欢汽车，以前只能玩玩游戏,以后可以直接上路了！😄🤩\n本想记录一下学车日常的，但是唯结果论了😅😅。从7月9日至8月6日，驾照终于到手，也算是完成了一个儿时的愿望，有点小激动，特此发文纪念一下。🚗\n","date":"2024年8月6日","permalink":"/zh-cn/life/driving_lessons/","section":"生活","summary":"驾照到手，天下我有！","title":"暑期学车日常"},{"content":"","date":null,"permalink":"/zh-cn/tags/ai/","section":"Tags","summary":"","title":"AI"},{"content":"","date":null,"permalink":"/zh-cn/tags/ml/","section":"Tags","summary":"","title":"ML"},{"content":"","date":null,"permalink":"/zh-cn/tags/pytorch/","section":"Tags","summary":"","title":"Pytorch"},{"content":"1 数据操作 #1.1 数据操作 ## 数据操作 import torch x = torch.arange(12) x tensor([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11])\rx.shape torch.Size([12])\rx.numel() 12\rx = x.reshape(3, 4) x tensor([[ 0, 1, 2, 3],\r[ 4, 5, 6, 7],\r[ 8, 9, 10, 11]])\rtorch.zeros((2, 3, 4)) tensor([[[0., 0., 0., 0.],\r[0., 0., 0., 0.],\r[0., 0., 0., 0.]],\r[[0., 0., 0., 0.],\r[0., 0., 0., 0.],\r[0., 0., 0., 0.]]])\rtorch.ones((2, 3, 4)) tensor([[[1., 1., 1., 1.],\r[1., 1., 1., 1.],\r[1., 1., 1., 1.]],\r[[1., 1., 1., 1.],\r[1., 1., 1., 1.],\r[1., 1., 1., 1.]]])\rtorch.tensor([[1, 2, 3, 4], [2, 1, 3, 4], [3, 4, 1, 2]]).shape torch.Size([3, 4])\rx = torch.tensor([1.0, 2, 4, 8]) y = torch.tensor([2, 2, 2, 2]) x + y, x - y, x * y, x / y, x ** y (tensor([ 3., 4., 6., 10.]),\rtensor([-1., 0., 2., 6.]),\rtensor([ 2., 4., 8., 16.]),\rtensor([0.5000, 1.0000, 2.0000, 4.0000]),\rtensor([ 1., 4., 16., 64.]))\rtorch.exp(x) tensor([2.7183e+00, 7.3891e+00, 5.4598e+01, 2.9810e+03])\rX = torch.arange(12, dtype=torch.float32).reshape(3, 4) Y = torch.tensor([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]]) torch.cat((X, Y), dim=0), torch.cat((X, Y), dim=1) (tensor([[ 0., 1., 2., 3.],\r[ 4., 5., 6., 7.],\r[ 8., 9., 10., 11.],\r[ 2., 1., 4., 3.],\r[ 1., 2., 3., 4.],\r[ 4., 3., 2., 1.]]),\rtensor([[ 0., 1., 2., 3., 2., 1., 4., 3.],\r[ 4., 5., 6., 7., 1., 2., 3., 4.],\r[ 8., 9., 10., 11., 4., 3., 2., 1.]]))\rX == Y tensor([[False, True, False, True],\r[False, False, False, False],\r[False, False, False, False]])\rX.sum() tensor(66.)\ra = torch.arange(3).reshape(3, 1) b = torch.arange(2) a, b # 同型矩阵运算为各元素分别计算，不同型矩阵运算先拓展到二者的最小公约类型再计算 (tensor([[0],\r[1],\r[2]]),\rtensor([0, 1]))\ra + b tensor([[0, 1],\r[1, 2],\r[2, 3]])\rX[-1], X[1:3] # X[1:3]，表示X的1、2两行（从0开始命名），左闭右开 (tensor([ 8., 9., 10., 11.]),\rtensor([[12., 12., 12., 12.],\r[ 8., 9., 10., 11.]]))\rX[1, 2] = 9 X tensor([[ 0., 1., 2., 3.],\r[ 4., 5., 9., 7.],\r[ 8., 9., 10., 11.]])\rX[0:2, :] = 12 X tensor([[12., 12., 12., 12.],\r[12., 12., 12., 12.],\r[ 8., 9., 10., 11.]])\rbefore = id(Y) Y += X id(Y) == before True\rY = Y + X id(Y) == before False\rZ = torch.zeros_like(Y) print(\u0026#34;id(Z): \u0026#34;, id(Z)) Z[:] = X + Y # 执行原地操作的办法 print(\u0026#34;id(Z): \u0026#34;, id(Z)) id(Z): 1932872841072\rid(Z): 1932872841072\rA = X.numpy() B = torch.tensor(A) type(A), type(B) (numpy.ndarray, torch.Tensor)\ra = torch.tensor([3.5]) a, a.item(), float(a), int(a) (tensor([3.5000]), 3.5, 3.5, 3)\r1.2 数据预处理 #import os os.makedirs(os.path.join(\u0026#39;\u0026#39;, \u0026#39;data\u0026#39;), exist_ok=True) data_file = os.path.join(\u0026#39;\u0026#39;, \u0026#39;data\u0026#39;, \u0026#39;house_tiny.csv\u0026#39;) with open(data_file, \u0026#39;w\u0026#39;) as f: f.write(\u0026#39;NumRooms, Alley, Price\\n\u0026#39;) f.write(\u0026#39;NA, Pave, 127500\\n\u0026#39;) f.write(\u0026#39;2, NA, 106000\\n\u0026#39;) f.write(\u0026#39;4, NA, 178100\\n\u0026#39;) f.write(\u0026#39;NA, NA, 140000\\n\u0026#39;) import pandas as pd data = pd.read_csv(data_file) print(data) NumRooms Alley Price\r0 NaN Pave 127500\r1 2.0 NA 106000\r2 4.0 NA 178100\r3 NaN NA 140000\rinputs, outputs = data.iloc[:, 0:2], data.iloc[:, 2] # iloc函数是用来取行或列 inputs = inputs.fillna(inputs.mean(numeric_only=True)) # fillna是填补NA的常见函数，这里采用均值插值法。注意此处必须添加numeric_only=True，因为只有数值才能计算均值 print(inputs) NumRooms Alley\r0 3.0 Pave\r1 2.0 NA\r2 4.0 NA\r3 3.0 NA\rinputs = pd.get_dummies(inputs, dummy_na=True) # 对于离散型的数值（如字符等），可采用get_dummies函数将其转化为数值型编码 print(inputs) NumRooms Alley_ NA Alley_ Pave Alley_nan\r0 3.0 False True False\r1 2.0 True False False\r2 4.0 True False False\r3 3.0 True False False\r2 线性代数 #2.1 矩阵计算 ## 矩阵操作 import torch A = torch.arange(20, dtype=torch.float32).reshape(5, 4) A tensor([[ 0., 1., 2., 3.],\r[ 4., 5., 6., 7.],\r[ 8., 9., 10., 11.],\r[12., 13., 14., 15.],\r[16., 17., 18., 19.]])\rA.T tensor([[ 0., 4., 8., 12., 16.],\r[ 1., 5., 9., 13., 17.],\r[ 2., 6., 10., 14., 18.],\r[ 3., 7., 11., 15., 19.]])\rx = torch.arange(4, dtype=torch.float32) y = torch.ones(4, dtype=torch.float32) x, y, torch.dot(x, y) # dot为点乘函数 (tensor([0., 1., 2., 3.]), tensor([1., 1., 1., 1.]), tensor(6.))\rtorch.sum(x * y) tensor(6.)\rA.shape, x.shape, torch.mv(A, x) # m表示matrix，v表示vector，mv就是矩阵乘向量 (torch.Size([5, 4]), torch.Size([4]), tensor([ 14., 38., 62., 86., 110.]))\rB = torch.ones(4, 3) torch.mm(A, B) # 同理，mm表示两个矩阵相乘 tensor([[ 6., 6., 6.],\r[22., 22., 22.],\r[38., 38., 38.],\r[54., 54., 54.],\r[70., 70., 70.]])\r范数\n\\(L_{2}\\)范数就是向量平方根，可以用norm函数直接计算：\n$$ \\Vert X \\Vert_{2} = \\sqrt{\\sum_{i=1}^n {x_{i}}^2} $$\nu = torch.tensor([3.0, -4.0]) torch.norm(u) tensor(5.)\r\\(L_{1}\\)范数则是向量元素绝对值之和： $$ \\Vert X \\Vert_{1} = \\sum_{i=1}^n {x_{i}} $$\ntorch.abs(u).sum() tensor(7.)\r矩阵最常见的就是佛罗贝尼乌斯范数，即矩阵元素的平方和的平方根： $$ \\Vert X \\Vert_{F} = \\sqrt{\\sum_{i=1}^m \\sum_{j=1}^n {x_{ij}}^2} $$\ntorch.norm(torch.ones(4, 9)) tensor(6.)\r# 按轴求和 a = torch.ones(2, 5, 4) a.shape torch.Size([2, 5, 4])\ra.sum(axis=[0, 2]).shape torch.Size([5])\ra.sum(axis=[0, 2], keepdims=True) # keepdims用来保存矩阵维度，即令被求和的维度以维度1保存在矩阵中，方便后续计算 tensor([[[8.],\r[8.],\r[8.],\r[8.],\r[8.]]])\r2.2 梯度 #对于\\(\\frac{\\partial y}{\\partial x}\\) ，我们可以对y和x分别分标量和矢量情况来讨论。\n(1) x为矢量，y为标量（分母布局符号）\n设\\(\\textbf{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ x_4 \\\\ x_5 \\\\ \\end{bmatrix}\\)，则\\(\\frac{\\partial y}{\\partial \\textbf{x}} = \\begin{bmatrix} \\frac{\\partial y}{\\partial x_1} \u0026amp; \\frac{\\partial y}{\\partial x_2} \u0026amp; \\frac{\\partial y}{\\partial x_3} \u0026amp; \\frac{\\partial y}{\\partial x_4} \u0026amp; \\frac{\\partial y}{\\partial x_5} \\end{bmatrix}\\)\n(2) x为标量，y为矢量（分子布局符号）\n设\\(\\textbf{y} = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ y_3 \\\\ \u0026hellip; \\\\ y_m \\\\ \\end{bmatrix}\\)，则\\(\\frac{\\partial \\textbf{y}}{\\partial x} = \\begin{bmatrix} \\frac{\\partial y_1}{\\partial x} \\\\ \\frac{\\partial y_2}{\\partial x} \\\\ \\frac{\\partial y_3}{\\partial x} \\\\ \u0026hellip; \\\\ \\frac{\\partial y_m}{\\partial x} \\\\ \\end{bmatrix}\\)\n(3) x，y都为矢量\n设\\(\\textbf{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ \u0026hellip; \\\\ x_n \\\\ \\end{bmatrix}\\)，\\(\\textbf{y} = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ y_3 \\\\ \u0026hellip; \\\\ y_m \\\\ \\end{bmatrix}\\)，则 \\(\\frac{\\partial \\textbf{y}}{\\partial \\textbf{x}} = \\begin{bmatrix} \\frac{\\partial y_1}{\\partial x_1} \u0026amp; \\frac{\\partial y_1}{\\partial x_2} \u0026amp; \u0026hellip; \u0026amp; \\frac{\\partial y_1}{\\partial x_n} \\\\ \\frac{\\partial y_2}{\\partial x_1} \u0026amp; \\frac{\\partial y_2}{\\partial x_2} \u0026amp; \u0026hellip; \u0026amp; \\frac{\\partial y_2}{\\partial x_n} \\\\ \u0026amp;\u0026amp;\u0026hellip; \\\\ \\frac{\\partial y_m}{\\partial x_1} \u0026amp; \\frac{\\partial y_m}{\\partial x_2} \u0026amp; \u0026hellip; \u0026amp; \\frac{\\partial y_m}{\\partial x_n} \\\\ \\end{bmatrix}\\)\n2.3 自动求导 #Python的求导方式主要分为隐式和显式两种。Pytorch使用的为隐式构造，而Tensorflow使用的是显式构造。隐式构造通常更加方便且节约存储资源。\n下面第一个例子中展示的是\\(y = 2 \\textbf{x}^T \\textbf{x}\\)对\\(\\textbf{x}\\)的求导过程。\nimport torch x = torch.arange(4.0) x tensor([0., 1., 2., 3.])\rx.requires_grad_(True) # 表示需要存储计算出来的导数 x.grad # 用来查看存储的导数值，默认为None y = 2 * torch.dot(x, x) # dot表示点乘，这里表示x的转置乘x y tensor(28., grad_fn=\u0026lt;MulBackward0\u0026gt;)\ry.backward() # 调用反向传播函数来求导 x.grad tensor([ 0., 4., 8., 12.])\rx.grad == 4 * x tensor([True, True, True, True])\rx.grad.zero_() # 清除之前计算的梯度 y = x.sum() y.backward() x.grad tensor([1., 1., 1., 1.])\r深度学习中通常不直接让向量对向量求导，而是将向量转化为标量后再对向量求导，因为向量对向量求导需要在backward函数中引入一个gradient参数，该参数指定微分函数关于self的梯度。\nx.grad.zero_() y = x * x y.sum().backward() # 相当于y.backward(torch.ones(len(x)))。 x.grad tensor([0., 2., 4., 6.])\rx.grad.zero_() y = x * x u = y.detach() # 表示将y的值赋给u，并且解除u和x之间的关系 z = u * x z.sum().backward() x.grad == u tensor([True, True, True, True])\rx.grad.zero_() y.sum().backward() x.grad == 2 * x tensor([True, True, True, True])\rdef f(a): b = a * 2 while b.norm() \u0026lt; 1000: b *= 2 if b.sum() \u0026gt; 0: c = b else: c = 100 * b return c a = torch.randn(size=(), requires_grad=True) # size=()表示标量 d = f(a) d.backward() a.grad == d / a tensor(True)\r3 基础回归算法 #3.1 线性回归 #对于一个给定的n维输入\\(x = \\begin{bmatrix}x_1,x_2,\u0026hellip;,x_n\\end{bmatrix}^T\\)，线性模型有一个n维权重\\(w = \\begin{bmatrix}w_1,w_2,\u0026hellip;,w_n\\end{bmatrix}^T\\)和一个标量偏差b。其数学表达式为： $$ y = w_1x_1 + w_2x_2 + \u0026hellip; + w_nx_n + b $$\n向量形式为： $$ y = \\lang{\\textbf{w}, \\textbf{x}}\\rang + b $$\n可以看作单层神经网络： 当令b取平方损失\\(\\ell(y, \\hat{y}) = \\frac 1 2 (y - \\hat{y})^2\\)时，训练损失可以表示为： $$ \\ell(\\textbf{X}, \\textbf{y}, \\textbf{w}, b) = \\frac 1 {2n} \\sum_{i=1}^{2n} (y_i - \\lang{\\mathbf{x_i}, \\textbf{w}}\\rang - b)^2 = \\frac 1 {2n} \\lVert{\\textbf{y} - \\textbf{Xw} - b}\\rVert $$\n令\\(\\textbf{X} = [\\textbf{X}, 1], \\textbf{w} = \\begin{bmatrix}\\textbf{w}\\\\b\\end{bmatrix}\\)，则\\(\\ell(\\textbf{X}, \\textbf{y}, \\textbf{w}) = \\frac 1 {2n} \\lVert{\\textbf{y} - \\textbf{Xw}}\\rVert^2\\).\n由于上式为凹函数，故对w求导，可取出损失的最小值，此时的w（即原来的w和b）取最小值。 $$ \\frac \\partial {\\partial\\textbf{w}} \\ell(\\textbf{X}, \\textbf{y}, \\textbf{w}) = 0\\\\\\\\ \\Leftrightarrow \\frac 1 n (\\textbf{y} - \\textbf{Xw})^T\\textbf{X} = 0\\\\\\\\ \\Leftrightarrow \\mathbf{w}^* = (\\mathbf{X}^T\\textbf{X})^{-1}\\mathbf{X}^T\\textbf{y} $$\n线性回归都有显示解，但大多数深度学习模型都没有显示解，因此不能通过简单求导得出结论，一般可以通过一些优化算法来求解，如下所示为基础优化方法（梯度下降法）。\n下面的例子中，我们通过给定随机的一组二维向量X、公式\\(y = \\textbf{Xw} + b + \\epsilon\\)（\\(\\epsilon\\)为随机噪声参数，服从标准正态分布）和y值，来预测线性回归的参数w和b，采用梯度下降的优化方法。\n%matplotlib inline # 嵌入matplotlib import random import torch from d2l import torch as d2l def synthetic_data(w, b, num_examples): # 生成y = Xw + b + 噪声 X = torch.normal(0, 1, (num_examples, len(w))) # normal为正态分布函数，此处构建num_examples个维度为w长度的服从标准正态分布的X值 y = torch.matmul(X, w) + b # matmul就是矩阵乘法 y += torch.normal(0, 0.01, y.shape) return X, y.reshape(-1, 1) true_w = torch.tensor([2, -3.4]) true_b = 4.2 features, labels = synthetic_data(true_w, true_b, 1000) print(\u0026#39;features: \u0026#39;, features[0], \u0026#39;\\nlabel: \u0026#39;, labels[0]) features: tensor([0.8040, 1.4655]) label: tensor([0.8383])\rd2l.set_figsize() d2l.plt.scatter(features[:, 1].detach().numpy(), labels.detach().numpy(), 1) \u0026lt;matplotlib.collections.PathCollection at 0x25f0d8d5450\u0026gt;\rdef data_iter(batch_size, features, labels): num_examples = len(features) indices = list(range(num_examples)) # 生成list random.shuffle(indices) # 打乱下标 for i in range(0, num_examples, batch_size): batch_indices = torch.tensor(indices[i:min(i + batch_size, num_examples)]) yield features[batch_indices], labels[batch_indices] # yield相当于return，属于惰性计算 batch_size = 10 for X, y in data_iter(batch_size, features, labels): print(X, \u0026#39;\\n\u0026#39;, y) break tensor([[ 0.5823, 1.3700],\r[-0.0490, -1.3720],\r[-0.7192, 0.1148],\r[-0.0823, -0.0522],\r[ 3.1452, -0.4425],\r[ 1.5709, 0.9346],\r[-1.4497, 1.3512],\r[-0.6982, -1.0002],\r[-0.3893, -0.1003],\r[ 1.8286, -0.7445]]) tensor([[ 0.7096],\r[ 8.7721],\r[ 2.3609],\r[ 4.2116],\r[11.9909],\r[ 4.1835],\r[-3.2944],\r[ 6.1959],\r[ 3.7565],\r[10.3933]])\r# 为w和b进行随机初始化 w = torch.normal(0, 0.01, size=(2, 1), requires_grad=True) # 因为X的size为[n, 2]，所以w的size为[2, 1] b = torch.zeros(1, requires_grad=True) def linreg(X, w, b): return torch.matmul(X, w) + b 线性回归常使用均方损失函数来计算预测值与真实值的偏差，均方损失（L2 Loss）函数的公式为： $$ l(y, y^{\\prime}) = \\frac 1 2 (y - y^{\\prime})^2 $$\n之所以有一个\\(\\frac 1 2\\)，是为了方便求导。\ndef squared_loss(y_hat, y): # 均方损失 return (y_hat - y.reshape(y_hat.shape))**2 / 2 def sgd(params, lr, batch_size): # 小批量随机梯度下降 with torch.no_grad(): # no_grad函数表示接下来不要记录计算的梯度 for param in params: param -= lr * param.grad / batch_size param.grad.zero_() lr = 0.03 num_epochs = 3 net = linreg loss = squared_loss for epoch in range(num_epochs): for X, y in data_iter(batch_size, features, labels): l = loss(net(X, w, b), y) # 计算预测的y和真实的y的均方损失 l.sum().backward() # 因为l是一个[batch_size, 1]大小的矢量，故对其求和以计算梯度 sgd([w, b], lr, batch_size) # 使用sgd函数进行梯度下降 with torch.no_grad(): train_l = loss(net(features, w, b), labels) print(f\u0026#39;epoch {epoch + 1}, loss {float(train_l.mean()): f}\u0026#39;) epoch 1, loss 0.040788\repoch 2, loss 0.000154\repoch 3, loss 0.000051\rprint(f\u0026#39;w的估计误差：{true_w - w.reshape(true_w.shape)}\u0026#39;) print(f\u0026#39;b的估计误差：{true_b - b}\u0026#39;) w的估计误差：tensor([5.7340e-05, 1.0276e-04], grad_fn=\u0026lt;SubBackward0\u0026gt;)\rb的估计误差：tensor([-0.0003], grad_fn=\u0026lt;RsubBackward1\u0026gt;)\r由于很多函数已经被封装在Pytorch等库中，故每次不需要如上复杂般进行，上述代码可简化如下：\nimport numpy as np import torch from torch.utils import data from d2l import torch as d2l true_w = torch.tensor([2, -3.4]) true_b = 4.2 features, labels = d2l.synthetic_data(true_w, true_b, 1000) def load_array(data_arrays, batch_size, is_train = True): # 构造一个Pytorch的数据迭代器 dataset = data.TensorDataset(*data_arrays) return data.DataLoader(dataset, batch_size, shuffle=is_train) batch_size = 10 data_iter = load_array((features, labels), batch_size) next(iter(data_iter)) [tensor([[-0.4498, -0.7477],\r[ 0.6690, 2.2404],\r[ 0.1602, 1.2009],\r[ 0.8746, 1.9034],\r[ 1.0719, 1.7307],\r[-1.4780, -2.2426],\r[ 0.7266, -1.5096],\r[-1.1183, -0.2499],\r[-0.9855, -0.6008],\r[ 0.9017, 0.9173]]),\rtensor([[ 5.8516],\r[-2.0883],\r[ 0.4374],\r[-0.5186],\r[ 0.4520],\r[ 8.8814],\r[10.7827],\r[ 2.8048],\r[ 4.2726],\r[ 2.8904]])]\rfrom torch import nn net = nn.Sequential(nn.Linear(2, 1)) net[0].weight.data.normal_(0, 0.01) net[0].bias.data.fill_(0) tensor([0.])\rloss = nn.MSELoss() # 定义均方误差实类 trainer = torch.optim.SGD(net.parameters(), lr = 0.03) # 实例化SGD实例 num_epochs = 3 for epoch in range(num_epochs): for X, y in data_iter: l = loss(net(X), y) trainer.zero_grad() l.backward() trainer.step() l = loss(net(features), labels) print(f\u0026#39;epoch {epoch + 1}, loss {l:f}\u0026#39;) epoch 1, loss 0.000101\repoch 2, loss 0.000102\repoch 3, loss 0.000101\r3.2 Softmax回归 #分类和回归的区别： Softmax回归本质上是一个多类分类模型，其计算公式如下： $$ \\hat{y_i} = \\frac {exp(o_i)} {\\sum_{k} exp(o_k)} $$\nSoftmax函数常与交叉熵损失联用，后者用来衡量两个概率之间的区别，常用于二分类或多分类问题。交叉熵损失函数为： $$ l(y, \\hat{y}) = - \\sum_{i} y_i log\\hat{y_i} = -log\\hat{y_y} $$ 故其梯度刚好是Softmax预测值与真实值的差值： $$ \\partial_{o_i}l(y, \\hat{y}) = softmax(o)_i - y_i $$ 从而可以通过梯度下降法进行优化。\n除了交叉熵损失函数和均方损失函数，L1 Loss也是一经典的损失函数： $$ l(y, y^{\\prime}) = |y - y^{\\prime}| $$\nL2函数适合于回归问题，但是当预测值于真实值较远时导数变化太大。L1函数虽然没有这个问题，但是在原点处不可导，因此当预测值与真实值接近时，变化不太稳定。结合以上二者的优势可以提出一个新的损失函数，如Huber\u0026rsquo;s Robust Loss函数。 $$ l(y, y^{\\prime}) = \\begin{cases} |y - y^{\\prime}| - \\frac 1 2 \u0026amp;\\text{if } |y - y^{\\prime}| \u0026gt; 1 \\\\\\\\ \\frac 1 2 (y - y^{\\prime})^2 \u0026amp;\\text{otherwise} \\end{cases} $$ 下面的代码为使用Pytorch框架读入数据集数据的示例。\n%matplotlib inline import torch import torchvision from torch.utils import data from torchvision import transforms from d2l import torch as d2l d2l.use_svg_display() # 使用svg显示图片以获得更高清晰度 trans = transforms.ToTensor() mnist_train = torchvision.datasets.FashionMNIST(root=\u0026#39;\u0026#39;, train=True, transform=trans, download=True) mnist_test = torchvision.datasets.FashionMNIST(root=\u0026#39;\u0026#39;, train=False, transform=trans, download=False) len(mnist_train), len(mnist_test) (60000, 10000)\rmnist_train[0][0].shape torch.Size([1, 28, 28])\rdef get_fashion_mnist_labels(labels): text_labels = [\u0026#39;t-shirt\u0026#39;, \u0026#39;trousers\u0026#39;, \u0026#39;pullover\u0026#39;, \u0026#39;dress\u0026#39;, \u0026#39;coat\u0026#39;, \u0026#39;sandal\u0026#39;, \u0026#39;shirt\u0026#39;, \u0026#39;sneaker\u0026#39;, \u0026#39;bag\u0026#39;, \u0026#39;ankle boot\u0026#39;] return [text_labels[int(i)] for i in labels] def show_images(imgs, num_rows, num_cols, titles=None, scale=1.5): figsize = (num_cols * scale, num_rows * scale) _, axes = d2l.plt.subplots(num_rows, num_cols, figsize=figsize) axes = axes.flatten() for i, (ax, img) in enumerate(zip(axes, imgs)): ax.set_title(titles[i]) ax.axis(\u0026#39;off\u0026#39;) if torch.is_tensor(img): ax.imshow(img.numpy()) else: ax.imshow(img) X, y = next(iter(data.DataLoader(mnist_train, batch_size=18))) show_images(X.reshape(18, 28, 28), 2, 9, titles=get_fashion_mnist_labels(y)) batch_size = 256 def get_dataloader_workers(): return 4 # 使用4个进程来读取数据 train_iter = data.DataLoader(mnist_train, batch_size, shuffle=True, num_workers=get_dataloader_workers()) timer = d2l.Timer() # 使用Timer函数计算数据读入时间 for X, y in train_iter: continue f\u0026#39;{timer.stop():.2f} sec\u0026#39; '3.19 sec'\r上述操作可以归并至一起，如下所示。\ndef load_data_fashion_mnist(batch_size, resize=None): trans = [transforms.ToTensor()] if resize: trans.insert(0, transforms.Resize(resize)) trans = transforms.Compose(trans) mnist_train = torchvision.datasets.FashionMNIST(root=\u0026#39;\u0026#39;, train=True, transform=trans, download=True) mnist_test = torchvision.datasets.FashionMNIST(root=\u0026#39;\u0026#39;, train=False, transform=trans, download=False) return (data.DataLoader(mnist_train, batch_size, shuffle=True, num_workers=get_dataloader_workers()), data.DataLoader(mnist_test, batch_size, shuffle=False, num_workers=get_dataloader_workers())) 下面是一段从0开始实现的Softmax回归的代码，由于这种代码过于复杂，故后续章节中我主要关注函数的简洁实现。\nimport torch from IPython import display from d2l import torch as d2l batch_size = 256 train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size) num_inputs = 784 num_outputs = 10 W = torch.normal(0, 0.01, size=(num_inputs, num_outputs), requires_grad=True) b = torch.zeros(num_outputs, requires_grad=True) X = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]]) X.sum(0, keepdim=True), X.sum(1, keepdim=True) (tensor([[5., 7., 9.]]),\rtensor([[ 6.],\r[15.]]))\rdef softmax(X): X_exp = torch.exp(X) partition = X_exp.sum(1, keepdim=True) return X_exp / partition # 此处应用了广播机制 X = torch.normal(0, 1, (2, 5)) X_prob = softmax(X) X_prob, X_prob.sum(1) (tensor([[0.5223, 0.0729, 0.0685, 0.2180, 0.1183],\r[0.6513, 0.1024, 0.0639, 0.1169, 0.0655]]),\rtensor([1., 1.]))\rdef net(X): # 使用X*W+b进行拟合 return softmax(torch.matmul(X.reshape((-1, W.shape[0])), W) + b) y = torch.tensor([0, 2]) y_hat = torch.tensor([[0.1, 0.3, 0.6], [0.3, 0.2, 0.5]]) y_hat[[0, 1], y] tensor([0.1000, 0.5000])\rdef cross_entropy(y_hat, y): # 交叉熵损失函数 return -torch.log(y_hat[range(len(y_hat)), y]) cross_entropy(y_hat, y) tensor([2.3026, 0.6931])\rdef accuracy(y_hat, y): # 计算预测正确的数量 if len(y_hat.shape) \u0026gt; 1 and y_hat.shape[1] \u0026gt; 1: y_hat = y_hat.argmax(axis=1) cmp = y_hat.type(y.dtype) == y return float(cmp.type(y.dtype).sum()) accuracy(y_hat, y) / len(y) 0.5\rdef evaluate_accuracy(net, data_iter): if isinstance(net, torch.nn.Module): # isinstance函数是用来判断一个对象是否是指定类的实例，此处判断net是否为torch.nn.Module的实例，即net是否为一个神经网络模型 net.eval() # 将模型设置为评估模式 metric = Accumulator(2) for X, y in data_iter: metric.add(accuracy(net(X), y), y.numel()) return metric[0] / metric[1] class Accumulator: def __init__(self, n): self.data = [0, 0] * n def add(self, *args): self.data = [a + float(b) for a, b in zip(self.data, args)] def reset(self): self.data = [0.0] * len(self.data) def __getitem__(self, idx): return self.data[idx] evaluate_accuracy(net, test_iter) 0.1208\rdef train_epoch_ch3(net, train_iter, loss, updater): if isinstance(net, torch.nn.Module): net.train() metric = Accumulator(3) for X, y in train_iter: y_hat = net(X) l = loss(y_hat, y) if isinstance(updater, torch.optim.Optimizer): updater.zero_grad() l.backward() updater.step() metric.add(float(l) * len(y), accuracy(y_hat, y), y.size().numel()) else: l.sum().backward() updater(X.shape[0]) metric.add(float(l.sum()), accuracy(y_hat, y), y.numel()) return metric[0] / metric[2], metric[1] / metric[2] class Animator: def __init__(self, xlabel=None, ylabel=None, legend=None, xlim=None, ylim=None, xscale=\u0026#39;linear\u0026#39;, yscale=\u0026#39;linear\u0026#39;, fmts=(\u0026#39;-\u0026#39;, \u0026#39;m--\u0026#39;, \u0026#39;g-.\u0026#39;, \u0026#39;r:\u0026#39;), nrows=1, ncols=1, figsize=(3.5, 2.5)): if legend is None: legend = [] d2l.use_svg_display() self.fig, self.axes = d2l.plt.subplots(nrows, ncols, figsize=figsize) if nrows * ncols == 1: self.axes = [self.axes, ] self.config_axes = lambda: d2l.set_axes(self.axes[0], xlabel, ylabel, xlim, ylim, xscale, yscale, legend) self.X, self.Y, self.fmts = None, None, fmts def add(self, x, y): if not hasattr(y, \u0026#34;__len__\u0026#34;): y = [y] n = len(y) if not hasattr(x, \u0026#34;__len__\u0026#34;): x = [x] * n if not self.X: self.X = [[] for _ in range(n)] if not self.Y: self.Y = [[] for _ in range(n)] for i, (a, b) in enumerate(zip(x, y)): if a is not None and b is not None: self.X[i].append(a) self.Y[i].append(b) self.axes[0].cla() for x, y, fmt in zip(self.X, self.Y, self.fmts): self.axes[0].plot(x, y, fmt) self.config_axes() display.display(self.fig) display.clear_output(wait=True) def train_ch3(net, train_iter, test_iter, loss, num_epochs, updater): animator = Animator(xlabel=\u0026#39;epoch\u0026#39;, xlim=[1, num_epochs], ylim=[0.3, 0.9], legend=[\u0026#39;train loss\u0026#39;, \u0026#39;train acc\u0026#39;, \u0026#39;test acc\u0026#39;]) for epoch in range(num_epochs): train_metrics = train_epoch_ch3(net, train_iter, loss, updater) test_acc = evaluate_accuracy(net, test_iter) animator.add(epoch + 1, train_metrics + (test_acc, )) train_loss, train_acc = train_metrics lr = 0.1 def updater(batch_size): return d2l.sgd([W, b], lr, batch_size) num_epochs = 10 train_ch3(net, train_iter, test_iter, cross_entropy, num_epochs, updater) def predict_ch3(net, test_iter, n=6): for X, y in test_iter: break trues = d2l.get_fashion_mnist_labels(y) preds = d2l.get_fashion_mnist_labels(net(X).argmax(axis=1)) titles = [true + \u0026#39;\\n\u0026#39; + pred for true, pred in zip(trues, preds)] d2l.show_images(X[0:n].reshape(n, 28, 28), 1, n, titles=titles[0:n]) predict_ch3(net, test_iter) Softmax回归精简版：\nimport torch from torch import nn from d2l import torch as d2l batch_size = 256 train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size) net = nn.Sequential(nn.Flatten(), nn.Linear(784, 10)) def init_weights(m): if type(m) == nn.Linear: nn.init.normal_(m.weight, std=0.01) # std为标准差参数 net.apply(init_weights) Sequential(\r(0): Flatten(start_dim=1, end_dim=-1)\r(1): Linear(in_features=784, out_features=10, bias=True)\r)\rloss = nn.CrossEntropyLoss() trainer = torch.optim.SGD(net.parameters(), lr=0.1) num_epochs = 10 d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer) ","date":"2024年7月26日","permalink":"/zh-cn/study/pytorch/","section":"Studies","summary":"最近在跟着李沐大神学习Pytorch，以此记录下我所学习的一些知识。新手上路，还望多多指教。","title":"Pytorch学习笔记"},{"content":"","date":null,"permalink":"/zh-cn/study/","section":"Studies","summary":"","title":"Studies"},{"content":"考研结束后，我的内心抱有持久的伤痛，那些伤痛一直萦绕我的心头，直至大学毕业，谨以此文纪念我那最后的大学时光。\n西安 #去西安是去面试航空所（631所）的调剂。当初报名也仅是因为它是为数不多招收计算机科学技术（学硕）的调剂单位，而且报销往返交通、住宿费用。由此踏上前往西安的旅途。\n简单地说，面试黄了，但所幸在面试结束后当晚就收到了港科广的面试邀请，也算给了我最后的希望。\n临走的那天，我特意让司机绕了下陕西历史博物馆和大雁塔。看到大雁塔的瞬间，我不禁想起了十多年前的那个下晚，我随父母来西安旅游，仍是在旅游大巴车上遥望大雁塔。彼时的我或许永远也想不到我会以此方式再临西安吧。却不想因此错过了航班，只好在咸阳机场旁多住了一晚，为我这本就荒唐的出行更添一丝奇异。\n第二天一早，带着一丝疲惫，我终于离开了这个千年古都。\n广州 \u0026amp; 佛山 #很多时候，很多人，很多事都是在机缘巧合下发生的，而更多的时候，人都像一只无头苍蝇般在自己的人生轨迹上乱撞，以图撞出那尘封多年的枷锁。我来广州正是如此，也正如我报名港科广与北大软微一样，不过都是在跟风中妄图逆天改命罢了。只是事物多有联系性，环环相扣，最终还是逃不出这一环又一环。\n五月末的广州已然十分炎热，伴随着海洋性气候特有的湿润，很难让人在出行后不凝起一身汗珠。即使在冷气充足的空调房里，豆大的汗珠仍挂满我的额稍，令刚面试完的我略显病态。虽未可知结果如何，我已看得开些了。接下来的几天里，我虽无心出游，但躺在酒店里打游戏的心还是有的。与好友约上两局之后，无尽的空虚和恐惧又袭满我身。\n次日游了佛山，主要看了看祖庙，见了见黄飞鸿的徒子徒孙，稍吃了点顺德特产，便回去了。老实说，这些曾经令我痴迷多年的景点此刻竟难以提起我的一点兴趣。\n临行前，我特意去广州“小蛮腰”和海心桥看了看。我在心中默默想着，以后能来此处上学，日后来玩的机会还多着呢；倘若来不了，这遍我也无所谓了。\n从白云机场离开时，白云被乌云遮住了大部。\n毕业 #这个章节我本不想写的，毕竟破坏了我章节的有序性，但想到这毕竟也是我人生的一个转折点，此刻不写，日后也难有回忆了，便停不住笔墨，简单聊聊。\n简单而言，这时的我已经收到了港科广的录取通知书，因而也略怀开心地与同学们一起合影，照了很多毕业照。但是说有多开心，是绝对没有的。\n我曾无数次在睡觉前畅想过毕业典礼的场景，大约从我决定考研起，我就知道毕业典礼要么笑着过，要么苦笑着过。但此刻一想，我脑中终究还是响起了那首《我终于失去了你》。我恨我四年的努力化为泡影！我遗憾我过早地选择了考研！我伤痛我的四年如梦如幻、飞逝如电，却一事无成！正如我四年前走进这所学校一样。\n南京 \u0026amp; 镇江 \u0026amp; 扬州 #在前往这条线路前，我曾做过三次详细的计划，分别是河西走廊、顺江而下、北上辽东，但是都因汪凯瑞的因素耽误了，最终选择了这最朴实无华、最近、最短的江苏之旅。现下想想，人生又何尝不是如此呢？\n此三地我不想多加赘述，无非是白天出游，夜晚游戏。尽管前期准备很不爽，但是一起玩《文明6》的日子还是值得怀念的！\n以上为我2024上半年的年度总结，也是我大学的最终章。人生在世，苦多乐少。逆天改命，终归平凡。岂知人生何苦追忆往昔，我的一个时代终结了，我将这份记忆尘封于此，不再打开。\n","date":"2024年7月26日","permalink":"/zh-cn/life/graduation_season/","section":"生活","summary":"夜色微凉，不见那一抹残阳","title":"大学 · 最终章"},{"content":"1. Diffusion Model #","date":"2024年7月26日","permalink":"/zh-cn/study/nn/","section":"Studies","summary":"理论知识来源于周志华《机器学习》，我将在此页记录下我认为重要的知识点。","title":"机器学习"},{"content":"","date":null,"permalink":"/zh-cn/categories/","section":"Categories","summary":"","title":"Categories"},{"content":"我目前是一名香港科技大学（广州）的人工智能研究型硕士生。在此之前，我在浙江大学获得了农学学士学位。\n在本科学习期间，我参与了一些与农业机器人和昆虫图像识别相关的项目，这激发了我对人工智能的兴趣。在代码方面我是半路出家，仍然是个菜鸟🤡。\n我将在本博客中不定期更新我的生活🏃‍♂️、学习📖和科研🖥️相关。希望访问本网站的朋友可以多多指教，尽情讨论。👋🤣\n","date":null,"permalink":"/zh-cn/about/","section":"关于","summary":"","title":"关于"},{"content":"\r这是本博客的科研板块！🚀 我将记录一些我在香港科技大学（广州）攻读硕士期间将进行的研究。你可以在评论区尽情评论，或者如果你也有一些有趣的经历，欢迎你在评论区分享。⭐\r新的开始，新的科研！ ✊ #","date":null,"permalink":"/zh-cn/research/","section":"科研","summary":"","title":"科研"}]
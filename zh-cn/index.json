[{"content":"","date":null,"permalink":"/zh-cn/tags/life/","section":"Tags","summary":"","title":"Life"},{"content":"","date":null,"permalink":"/zh-cn/tags/","section":"Tags","summary":"","title":"Tags"},{"content":"","date":null,"permalink":"/zh-cn/","section":"Ystars","summary":"","title":"Ystars"},{"content":"\rè¿™æ˜¯æœ¬åšå®¢çš„ç”Ÿæ´»æ¿å—ï¼ğŸš€å®ƒå°†ç”¨äºåˆ†äº«æˆ‘å‘¨å›´çš„ä¸€äº›æœ‰è¶£çš„äº‹æƒ…ã€‚ä½ å¯ä»¥åœ¨è¯„è®ºåŒºå°½æƒ…è¯„è®ºï¼Œæˆ–è€…å¦‚æœä½ ä¹Ÿæœ‰ä¸€äº›æœ‰è¶£çš„ç»å†ï¼Œæ¬¢è¿ä½ åœ¨è¯„è®ºåŒºåˆ†äº«ã€‚â­\rä¿æŒæ¯å¤©æ„‰å¿«å¿ƒæƒ…ï¼ ğŸ˜„ #","date":null,"permalink":"/zh-cn/life/","section":"ç”Ÿæ´»","summary":"","title":"ç”Ÿæ´»"},{"content":"å¾ˆä¹…æ²¡æœ‰å’Œçˆ¶æ¯ä¸€èµ·è¿œæ¸¸äº†ï¼Œè¿™åœ¨æˆ‘å°çš„æ—¶å€™æœ¬æ˜¯ä¸€ä»¶å¸¸æœ‰çš„äº‹ã€‚ç„¶è€Œéšç€æ—¶å…‰æ¶ˆé€ï¼Œè¿™ç§æ—…è¡Œç«Ÿä¹Ÿå˜å¾—è¶Šæ¥è¶Šå°‘ï¼Œä»¥è‡³äºå†æ¬¡æ—…è¡Œæ—¶ç«Ÿä¼šäº§ç”Ÿä¸€äº›åˆ«æ‰­çš„æƒ…ç»ªã€‚\nå¹¿å· #\rå¹¿å·æ˜¯æˆ‘ä»¬è¿™æ¬¡å»çš„ç¬¬ä¸€ä¸ªåœ°æ–¹ï¼Œå› ä¸ºæˆ‘å°†å°±è¯»çš„é¦™æ¸¯ç§‘æŠ€å¤§å­¦å¹¿å·æ ¡åŒºå°±åœ¨è¿™é‡Œã€‚å“¦å¯¹ï¼Œå¿˜è®°è¯´äº†ï¼Œè¿™æ¬¡æ—…è¡Œä¹Ÿæ˜¯æˆ‘çˆ¶æ¯é¡ºä¾¿ä¸ºæˆ‘é€è¡Œï¼Œé¡ºé“æ¥çœ‹çœ‹æˆ‘çš„æ–°å­¦æ ¡ã€‚è¿™æ˜¯æˆ‘ç¬¬äºŒæ¬¡æ¥åˆ°è¿™åº§åŸå¸‚ï¼Œè€Œæˆ‘çš„çˆ¶æ¯éƒ½æ˜¯ç¬¬ä¸€æ¬¡æ¥è¿™é‡Œã€‚\nåœ¨ç»å†äº†é£æœºæ™šç‚¹çš„ä¸€å¤œæŠ˜ç£¨åï¼Œæˆ‘å’Œçˆ¶æ¯ç²¾ç–²åŠ›å°½åœ°è¸ä¸Šäº†å¹¿å·è¿™ç‰‡åœŸåœ°ã€‚åœ¨å®¾é¦†ä¼‘æ•´å®Œæ¯•åï¼Œæˆ‘ä»¬ç§Ÿäº†ä¸€è¾†è½¦ï¼Œæ­£å¼å¼€å§‹äº†ä¸ºæœŸ2å¤©çš„å¹¿æ·±è‡ªé©¾æ¸¸ã€‚\nè¯´å®è¯ï¼Œèµ°éäº†ç¥–å›½çš„å¤šä¸ªåŸå¸‚åï¼Œè¿™äº›åŸå¸‚å¯¹æˆ‘å¹¶æ²¡æœ‰å¤šå°‘çš„å¸å¼•åŠ›ã€‚å°±æ‹¿å¹¿å·è€Œè¨€ï¼Œæ‰€å¸å¼•æˆ‘çš„ä¸è¿‡æ˜¯é‚£åœ°é“çš„â€œç²¤èœé¦†â€ä»¥åŠä¸€äº›æ°‘å›½æ—¶æœŸçš„å¤è€å»ºç­‘ã€‚åœ¨æ‰“å¡æ°¸åˆ©é¥­åº—ä»¥åï¼Œå¯¹ç²¤èœçš„æ„Ÿè§‰å´ä¹Ÿä¸å’¸ä¸æ·¡äº†ã€‚ç„¶è€Œå’Œçˆ¶æ¯ä¸€èµ·æ—…æ¸¸ï¼Œçºµç„¶å¤šäº›å£è§’ï¼Œå´ä¹Ÿå°‘äº›çƒ¦æ¼ã€‚æ€»ä½“è€Œè¨€ï¼Œåœ¨å¹¿å·çš„ä¸€å¤©è¿˜æ˜¯éå¸¸æ„‰å¿«çš„ã€‚\næ·±åœ³ #\rä»å—æ²™ç»è™é—¨ä¸Šæ·±åœ³ï¼Œæ”¾çœ¼æœ›å»ï¼Œæˆ‘å·²è¿·å¤±äºé‚£çƒŸé›¨ä¸­è€¸ç«‹çš„æ¥¼å®‡å¤§å¦ã€‚æ±½è½¦ç©¿æ¢­åœ¨é’¢é“ä¸›æ—ä¹‹ä¸­ï¼Œé“è·¯ä¸¤æ—çš†æ˜¯ä¸å¯çŸ¥å…¶é«˜çš„æ‘©å¤©å¤§æ¥¼ã€‚æ‰€è°“â€œCityâ€ï¼Œæ‰€è°“â€œæ‘©ç™»â€ï¼Œè«ä¸å¦‚æ˜¯ã€‚æˆ‘æ•…çŸ¥å¦‚æ­¤åŸå¸‚ï¼Œäº¦æœ‰é˜´æš—å¦‚åŸä¸­æ‘èˆ¬çš„å­˜åœ¨ï¼Œç„¶ä½œä¸ºä¸€ä¸ªæ¸¸å®¢ï¼Œåˆæœ‰å¤šå°‘äººä¼šå»ä¸“æ³¨äºé‚£äº›å¦‚åŒè‡ªå·±ä¸€æ ·å°˜åŸƒä¸­çš„äººä»¬å‘¢ï¼Ÿåœ¨æŸç§è§’åº¦ä¸Šæ¥è¯´ï¼Œæ·±åœ³æ˜¯ä¸€åº§è¿‘ä¹å®Œç¾çš„åŸå¸‚ï¼Œå®Œç¾å¾—ç”Ÿäººå‹¿è¿‘ã€‚\né¦™æ¸¯ #\rå¦‚æœè¯´æ·±åœ³æ˜¯ä¸€åº§æ–°ç§€çš„å®Œç¾é’å¹´ï¼Œé‚£ä¹ˆé¦™æ¸¯æˆ–è®¸æ˜¯ä¸€ä¸ªä¸æœè€çš„ä¸­å¹´äººã€‚æœŸé—´çš„åŸå› éš¾ä»¥è¨€è¯´ï¼Œä½†ä½œä¸ºä¸€ä¸ªæ¸¸å®¢ï¼Œä¸¤è¶Ÿå¤©æ˜Ÿå°è½®å°±å·²ç»è®©æˆ‘ä½“ä¼šåˆ°æœ¬æ¬¡æ—…è¡Œçš„æœ€å¤§ä¹è¶£äº†ï¼Œåˆä½•å¿…æ·±ç©¶ï¼Ÿ\nå¿«ä¹çš„æ—…é€”æ€»æ˜¯çŸ­æš‚çš„ï¼Œæˆ–è®¸æ¯ä¸€ä¸ªæƒ³è¦é•¿é€”æ—…æ¸¸çš„äººæ‰€æ±‚çš„éƒ½æ˜¯ä¸€ç‚¹è‡ªç”±ã€ä¸€ç‚¹æ–°å¥‡ï¼Œåˆæˆ–æ˜¯ä¸€ç‚¹ä¸ä¸€æ ·çš„ç”Ÿæ´»æ–¹å¼ï¼Œæ¥ç»™è¿™æœ¬å°±å¹³æ·¡æ— èŠçš„ç”Ÿæ´»å¢æ·»ä¸€ç‚¹åˆ«æ ·çš„ä¹è¶£ã€‚\næ–‡ç¬”éšæ„ï¼Œä»…ä¾›è‡ªå¨±ã€‚\n","date":"2024å¹´8æœˆ24æ—¥","permalink":"/zh-cn/life/travel_in_guangdong/","section":"ç”Ÿæ´»","summary":"ä¸€æ¬¡å€¼å¾—è¿½å¿†çš„æ—…ç¨‹\u0026hellip;\u0026hellip;","title":"ç ä¸‰è§’ä¹‹è¡Œ"},{"content":"ä¸€ç›´å–œæ¬¢æ±½è½¦ï¼Œä»¥å‰åªèƒ½ç©ç©æ¸¸æˆ,ä»¥åå¯ä»¥ç›´æ¥ä¸Šè·¯äº†ï¼ğŸ˜„ğŸ¤©\næœ¬æƒ³è®°å½•ä¸€ä¸‹å­¦è½¦æ—¥å¸¸çš„ï¼Œä½†æ˜¯å”¯ç»“æœè®ºäº†ğŸ˜…ğŸ˜…ã€‚ä»7æœˆ9æ—¥è‡³8æœˆ6æ—¥ï¼Œé©¾ç…§ç»ˆäºåˆ°æ‰‹ï¼Œä¹Ÿç®—æ˜¯å®Œæˆäº†ä¸€ä¸ªå„¿æ—¶çš„æ„¿æœ›ï¼Œæœ‰ç‚¹å°æ¿€åŠ¨ï¼Œç‰¹æ­¤å‘æ–‡çºªå¿µä¸€ä¸‹ã€‚ğŸš—\n","date":"2024å¹´8æœˆ6æ—¥","permalink":"/zh-cn/life/driving_lessons/","section":"ç”Ÿæ´»","summary":"é©¾ç…§åˆ°æ‰‹ï¼Œå¤©ä¸‹æˆ‘æœ‰ï¼","title":"æš‘æœŸå­¦è½¦æ—¥å¸¸"},{"content":"","date":null,"permalink":"/zh-cn/tags/ai/","section":"Tags","summary":"","title":"AI"},{"content":"","date":null,"permalink":"/zh-cn/tags/ml/","section":"Tags","summary":"","title":"ML"},{"content":"","date":null,"permalink":"/zh-cn/tags/pytorch/","section":"Tags","summary":"","title":"Pytorch"},{"content":"1 æ•°æ®æ“ä½œ #1.1 æ•°æ®æ“ä½œ ## æ•°æ®æ“ä½œ import torch x = torch.arange(12) x tensor([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11])\rx.shape torch.Size([12])\rx.numel() 12\rx = x.reshape(3, 4) x tensor([[ 0, 1, 2, 3],\r[ 4, 5, 6, 7],\r[ 8, 9, 10, 11]])\rtorch.zeros((2, 3, 4)) tensor([[[0., 0., 0., 0.],\r[0., 0., 0., 0.],\r[0., 0., 0., 0.]],\r[[0., 0., 0., 0.],\r[0., 0., 0., 0.],\r[0., 0., 0., 0.]]])\rtorch.ones((2, 3, 4)) tensor([[[1., 1., 1., 1.],\r[1., 1., 1., 1.],\r[1., 1., 1., 1.]],\r[[1., 1., 1., 1.],\r[1., 1., 1., 1.],\r[1., 1., 1., 1.]]])\rtorch.tensor([[1, 2, 3, 4], [2, 1, 3, 4], [3, 4, 1, 2]]).shape torch.Size([3, 4])\rx = torch.tensor([1.0, 2, 4, 8]) y = torch.tensor([2, 2, 2, 2]) x + y, x - y, x * y, x / y, x ** y (tensor([ 3., 4., 6., 10.]),\rtensor([-1., 0., 2., 6.]),\rtensor([ 2., 4., 8., 16.]),\rtensor([0.5000, 1.0000, 2.0000, 4.0000]),\rtensor([ 1., 4., 16., 64.]))\rtorch.exp(x) tensor([2.7183e+00, 7.3891e+00, 5.4598e+01, 2.9810e+03])\rX = torch.arange(12, dtype=torch.float32).reshape(3, 4) Y = torch.tensor([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]]) torch.cat((X, Y), dim=0), torch.cat((X, Y), dim=1) (tensor([[ 0., 1., 2., 3.],\r[ 4., 5., 6., 7.],\r[ 8., 9., 10., 11.],\r[ 2., 1., 4., 3.],\r[ 1., 2., 3., 4.],\r[ 4., 3., 2., 1.]]),\rtensor([[ 0., 1., 2., 3., 2., 1., 4., 3.],\r[ 4., 5., 6., 7., 1., 2., 3., 4.],\r[ 8., 9., 10., 11., 4., 3., 2., 1.]]))\rX == Y tensor([[False, True, False, True],\r[False, False, False, False],\r[False, False, False, False]])\rX.sum() tensor(66.)\ra = torch.arange(3).reshape(3, 1) b = torch.arange(2) a, b # åŒå‹çŸ©é˜µè¿ç®—ä¸ºå„å…ƒç´ åˆ†åˆ«è®¡ç®—ï¼Œä¸åŒå‹çŸ©é˜µè¿ç®—å…ˆæ‹“å±•åˆ°äºŒè€…çš„æœ€å°å…¬çº¦ç±»å‹å†è®¡ç®— (tensor([[0],\r[1],\r[2]]),\rtensor([0, 1]))\ra + b tensor([[0, 1],\r[1, 2],\r[2, 3]])\rX[-1], X[1:3] # X[1:3]ï¼Œè¡¨ç¤ºXçš„1ã€2ä¸¤è¡Œï¼ˆä»0å¼€å§‹å‘½åï¼‰ï¼Œå·¦é—­å³å¼€ (tensor([ 8., 9., 10., 11.]),\rtensor([[12., 12., 12., 12.],\r[ 8., 9., 10., 11.]]))\rX[1, 2] = 9 X tensor([[ 0., 1., 2., 3.],\r[ 4., 5., 9., 7.],\r[ 8., 9., 10., 11.]])\rX[0:2, :] = 12 X tensor([[12., 12., 12., 12.],\r[12., 12., 12., 12.],\r[ 8., 9., 10., 11.]])\rbefore = id(Y) Y += X id(Y) == before True\rY = Y + X id(Y) == before False\rZ = torch.zeros_like(Y) print(\u0026#34;id(Z): \u0026#34;, id(Z)) Z[:] = X + Y # æ‰§è¡ŒåŸåœ°æ“ä½œçš„åŠæ³• print(\u0026#34;id(Z): \u0026#34;, id(Z)) id(Z): 1932872841072\rid(Z): 1932872841072\rA = X.numpy() B = torch.tensor(A) type(A), type(B) (numpy.ndarray, torch.Tensor)\ra = torch.tensor([3.5]) a, a.item(), float(a), int(a) (tensor([3.5000]), 3.5, 3.5, 3)\r1.2 æ•°æ®é¢„å¤„ç† #import os os.makedirs(os.path.join(\u0026#39;\u0026#39;, \u0026#39;data\u0026#39;), exist_ok=True) data_file = os.path.join(\u0026#39;\u0026#39;, \u0026#39;data\u0026#39;, \u0026#39;house_tiny.csv\u0026#39;) with open(data_file, \u0026#39;w\u0026#39;) as f: f.write(\u0026#39;NumRooms, Alley, Price\\n\u0026#39;) f.write(\u0026#39;NA, Pave, 127500\\n\u0026#39;) f.write(\u0026#39;2, NA, 106000\\n\u0026#39;) f.write(\u0026#39;4, NA, 178100\\n\u0026#39;) f.write(\u0026#39;NA, NA, 140000\\n\u0026#39;) import pandas as pd data = pd.read_csv(data_file) print(data) NumRooms Alley Price\r0 NaN Pave 127500\r1 2.0 NA 106000\r2 4.0 NA 178100\r3 NaN NA 140000\rinputs, outputs = data.iloc[:, 0:2], data.iloc[:, 2] # ilocå‡½æ•°æ˜¯ç”¨æ¥å–è¡Œæˆ–åˆ— inputs = inputs.fillna(inputs.mean(numeric_only=True)) # fillnaæ˜¯å¡«è¡¥NAçš„å¸¸è§å‡½æ•°ï¼Œè¿™é‡Œé‡‡ç”¨å‡å€¼æ’å€¼æ³•ã€‚æ³¨æ„æ­¤å¤„å¿…é¡»æ·»åŠ numeric_only=Trueï¼Œå› ä¸ºåªæœ‰æ•°å€¼æ‰èƒ½è®¡ç®—å‡å€¼ print(inputs) NumRooms Alley\r0 3.0 Pave\r1 2.0 NA\r2 4.0 NA\r3 3.0 NA\rinputs = pd.get_dummies(inputs, dummy_na=True) # å¯¹äºç¦»æ•£å‹çš„æ•°å€¼ï¼ˆå¦‚å­—ç¬¦ç­‰ï¼‰ï¼Œå¯é‡‡ç”¨get_dummieså‡½æ•°å°†å…¶è½¬åŒ–ä¸ºæ•°å€¼å‹ç¼–ç  print(inputs) NumRooms Alley_ NA Alley_ Pave Alley_nan\r0 3.0 False True False\r1 2.0 True False False\r2 4.0 True False False\r3 3.0 True False False\r2 çº¿æ€§ä»£æ•° #2.1 çŸ©é˜µè®¡ç®— ## çŸ©é˜µæ“ä½œ import torch A = torch.arange(20, dtype=torch.float32).reshape(5, 4) A tensor([[ 0., 1., 2., 3.],\r[ 4., 5., 6., 7.],\r[ 8., 9., 10., 11.],\r[12., 13., 14., 15.],\r[16., 17., 18., 19.]])\rA.T tensor([[ 0., 4., 8., 12., 16.],\r[ 1., 5., 9., 13., 17.],\r[ 2., 6., 10., 14., 18.],\r[ 3., 7., 11., 15., 19.]])\rx = torch.arange(4, dtype=torch.float32) y = torch.ones(4, dtype=torch.float32) x, y, torch.dot(x, y) # dotä¸ºç‚¹ä¹˜å‡½æ•° (tensor([0., 1., 2., 3.]), tensor([1., 1., 1., 1.]), tensor(6.))\rtorch.sum(x * y) tensor(6.)\rA.shape, x.shape, torch.mv(A, x) # mè¡¨ç¤ºmatrixï¼Œvè¡¨ç¤ºvectorï¼Œmvå°±æ˜¯çŸ©é˜µä¹˜å‘é‡ (torch.Size([5, 4]), torch.Size([4]), tensor([ 14., 38., 62., 86., 110.]))\rB = torch.ones(4, 3) torch.mm(A, B) # åŒç†ï¼Œmmè¡¨ç¤ºä¸¤ä¸ªçŸ©é˜µç›¸ä¹˜ tensor([[ 6., 6., 6.],\r[22., 22., 22.],\r[38., 38., 38.],\r[54., 54., 54.],\r[70., 70., 70.]])\rèŒƒæ•°\n\\(L_{2}\\)èŒƒæ•°å°±æ˜¯å‘é‡å¹³æ–¹æ ¹ï¼Œå¯ä»¥ç”¨normå‡½æ•°ç›´æ¥è®¡ç®—ï¼š\n$$ \\Vert X \\Vert_{2} = \\sqrt{\\sum_{i=1}^n {x_{i}}^2} $$\nu = torch.tensor([3.0, -4.0]) torch.norm(u) tensor(5.)\r\\(L_{1}\\)èŒƒæ•°åˆ™æ˜¯å‘é‡å…ƒç´ ç»å¯¹å€¼ä¹‹å’Œï¼š $$ \\Vert X \\Vert_{1} = \\sum_{i=1}^n {x_{i}} $$\ntorch.abs(u).sum() tensor(7.)\rçŸ©é˜µæœ€å¸¸è§çš„å°±æ˜¯ä½›ç½—è´å°¼ä¹Œæ–¯èŒƒæ•°ï¼Œå³çŸ©é˜µå…ƒç´ çš„å¹³æ–¹å’Œçš„å¹³æ–¹æ ¹ï¼š $$ \\Vert X \\Vert_{F} = \\sqrt{\\sum_{i=1}^m \\sum_{j=1}^n {x_{ij}}^2} $$\ntorch.norm(torch.ones(4, 9)) tensor(6.)\r# æŒ‰è½´æ±‚å’Œ a = torch.ones(2, 5, 4) a.shape torch.Size([2, 5, 4])\ra.sum(axis=[0, 2]).shape torch.Size([5])\ra.sum(axis=[0, 2], keepdims=True) # keepdimsç”¨æ¥ä¿å­˜çŸ©é˜µç»´åº¦ï¼Œå³ä»¤è¢«æ±‚å’Œçš„ç»´åº¦ä»¥ç»´åº¦1ä¿å­˜åœ¨çŸ©é˜µä¸­ï¼Œæ–¹ä¾¿åç»­è®¡ç®— tensor([[[8.],\r[8.],\r[8.],\r[8.],\r[8.]]])\r2.2 æ¢¯åº¦ #å¯¹äº\\(\\frac{\\partial y}{\\partial x}\\) ï¼Œæˆ‘ä»¬å¯ä»¥å¯¹yå’Œxåˆ†åˆ«åˆ†æ ‡é‡å’ŒçŸ¢é‡æƒ…å†µæ¥è®¨è®ºã€‚\n(1) xä¸ºçŸ¢é‡ï¼Œyä¸ºæ ‡é‡ï¼ˆåˆ†æ¯å¸ƒå±€ç¬¦å·ï¼‰\nè®¾\\(\\textbf{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ x_4 \\\\ x_5 \\\\ \\end{bmatrix}\\)ï¼Œåˆ™\\(\\frac{\\partial y}{\\partial \\textbf{x}} = \\begin{bmatrix} \\frac{\\partial y}{\\partial x_1} \u0026amp; \\frac{\\partial y}{\\partial x_2} \u0026amp; \\frac{\\partial y}{\\partial x_3} \u0026amp; \\frac{\\partial y}{\\partial x_4} \u0026amp; \\frac{\\partial y}{\\partial x_5} \\end{bmatrix}\\)\n(2) xä¸ºæ ‡é‡ï¼Œyä¸ºçŸ¢é‡ï¼ˆåˆ†å­å¸ƒå±€ç¬¦å·ï¼‰\nè®¾\\(\\textbf{y} = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ y_3 \\\\ \u0026hellip; \\\\ y_m \\\\ \\end{bmatrix}\\)ï¼Œåˆ™\\(\\frac{\\partial \\textbf{y}}{\\partial x} = \\begin{bmatrix} \\frac{\\partial y_1}{\\partial x} \\\\ \\frac{\\partial y_2}{\\partial x} \\\\ \\frac{\\partial y_3}{\\partial x} \\\\ \u0026hellip; \\\\ \\frac{\\partial y_m}{\\partial x} \\\\ \\end{bmatrix}\\)\n(3) xï¼Œyéƒ½ä¸ºçŸ¢é‡\nè®¾\\(\\textbf{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ \u0026hellip; \\\\ x_n \\\\ \\end{bmatrix}\\)ï¼Œ\\(\\textbf{y} = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ y_3 \\\\ \u0026hellip; \\\\ y_m \\\\ \\end{bmatrix}\\)ï¼Œåˆ™ \\(\\frac{\\partial \\textbf{y}}{\\partial \\textbf{x}} = \\begin{bmatrix} \\frac{\\partial y_1}{\\partial x_1} \u0026amp; \\frac{\\partial y_1}{\\partial x_2} \u0026amp; \u0026hellip; \u0026amp; \\frac{\\partial y_1}{\\partial x_n} \\\\ \\frac{\\partial y_2}{\\partial x_1} \u0026amp; \\frac{\\partial y_2}{\\partial x_2} \u0026amp; \u0026hellip; \u0026amp; \\frac{\\partial y_2}{\\partial x_n} \\\\ \u0026amp;\u0026amp;\u0026hellip; \\\\ \\frac{\\partial y_m}{\\partial x_1} \u0026amp; \\frac{\\partial y_m}{\\partial x_2} \u0026amp; \u0026hellip; \u0026amp; \\frac{\\partial y_m}{\\partial x_n} \\\\ \\end{bmatrix}\\)\n2.3 è‡ªåŠ¨æ±‚å¯¼ #Pythonçš„æ±‚å¯¼æ–¹å¼ä¸»è¦åˆ†ä¸ºéšå¼å’Œæ˜¾å¼ä¸¤ç§ã€‚Pytorchä½¿ç”¨çš„ä¸ºéšå¼æ„é€ ï¼Œè€ŒTensorflowä½¿ç”¨çš„æ˜¯æ˜¾å¼æ„é€ ã€‚éšå¼æ„é€ é€šå¸¸æ›´åŠ æ–¹ä¾¿ä¸”èŠ‚çº¦å­˜å‚¨èµ„æºã€‚\nä¸‹é¢ç¬¬ä¸€ä¸ªä¾‹å­ä¸­å±•ç¤ºçš„æ˜¯\\(y = 2 \\textbf{x}^T \\textbf{x}\\)å¯¹\\(\\textbf{x}\\)çš„æ±‚å¯¼è¿‡ç¨‹ã€‚\nimport torch x = torch.arange(4.0) x tensor([0., 1., 2., 3.])\rx.requires_grad_(True) # è¡¨ç¤ºéœ€è¦å­˜å‚¨è®¡ç®—å‡ºæ¥çš„å¯¼æ•° x.grad # ç”¨æ¥æŸ¥çœ‹å­˜å‚¨çš„å¯¼æ•°å€¼ï¼Œé»˜è®¤ä¸ºNone y = 2 * torch.dot(x, x) # dotè¡¨ç¤ºç‚¹ä¹˜ï¼Œè¿™é‡Œè¡¨ç¤ºxçš„è½¬ç½®ä¹˜x y tensor(28., grad_fn=\u0026lt;MulBackward0\u0026gt;)\ry.backward() # è°ƒç”¨åå‘ä¼ æ’­å‡½æ•°æ¥æ±‚å¯¼ x.grad tensor([ 0., 4., 8., 12.])\rx.grad == 4 * x tensor([True, True, True, True])\rx.grad.zero_() # æ¸…é™¤ä¹‹å‰è®¡ç®—çš„æ¢¯åº¦ y = x.sum() y.backward() x.grad tensor([1., 1., 1., 1.])\ræ·±åº¦å­¦ä¹ ä¸­é€šå¸¸ä¸ç›´æ¥è®©å‘é‡å¯¹å‘é‡æ±‚å¯¼ï¼Œè€Œæ˜¯å°†å‘é‡è½¬åŒ–ä¸ºæ ‡é‡åå†å¯¹å‘é‡æ±‚å¯¼ï¼Œå› ä¸ºå‘é‡å¯¹å‘é‡æ±‚å¯¼éœ€è¦åœ¨backwardå‡½æ•°ä¸­å¼•å…¥ä¸€ä¸ªgradientå‚æ•°ï¼Œè¯¥å‚æ•°æŒ‡å®šå¾®åˆ†å‡½æ•°å…³äºselfçš„æ¢¯åº¦ã€‚\nx.grad.zero_() y = x * x y.sum().backward() # ç›¸å½“äºy.backward(torch.ones(len(x)))ã€‚ x.grad tensor([0., 2., 4., 6.])\rx.grad.zero_() y = x * x u = y.detach() # è¡¨ç¤ºå°†yçš„å€¼èµ‹ç»™uï¼Œå¹¶ä¸”è§£é™¤uå’Œxä¹‹é—´çš„å…³ç³» z = u * x z.sum().backward() x.grad == u tensor([True, True, True, True])\rx.grad.zero_() y.sum().backward() x.grad == 2 * x tensor([True, True, True, True])\rdef f(a): b = a * 2 while b.norm() \u0026lt; 1000: b *= 2 if b.sum() \u0026gt; 0: c = b else: c = 100 * b return c a = torch.randn(size=(), requires_grad=True) # size=()è¡¨ç¤ºæ ‡é‡ d = f(a) d.backward() a.grad == d / a tensor(True)\r3 åŸºç¡€å›å½’ç®—æ³• #3.1 çº¿æ€§å›å½’ #å¯¹äºä¸€ä¸ªç»™å®šçš„nç»´è¾“å…¥\\(x = \\begin{bmatrix}x_1,x_2,\u0026hellip;,x_n\\end{bmatrix}^T\\)ï¼Œçº¿æ€§æ¨¡å‹æœ‰ä¸€ä¸ªnç»´æƒé‡\\(w = \\begin{bmatrix}w_1,w_2,\u0026hellip;,w_n\\end{bmatrix}^T\\)å’Œä¸€ä¸ªæ ‡é‡åå·®bã€‚å…¶æ•°å­¦è¡¨è¾¾å¼ä¸ºï¼š $$ y = w_1x_1 + w_2x_2 + \u0026hellip; + w_nx_n + b $$\nå‘é‡å½¢å¼ä¸ºï¼š $$ y = \\lang{\\textbf{w}, \\textbf{x}}\\rang + b $$\nå¯ä»¥çœ‹ä½œå•å±‚ç¥ç»ç½‘ç»œï¼š å½“ä»¤bå–å¹³æ–¹æŸå¤±\\(\\ell(y, \\hat{y}) = \\frac 1 2 (y - \\hat{y})^2\\)æ—¶ï¼Œè®­ç»ƒæŸå¤±å¯ä»¥è¡¨ç¤ºä¸ºï¼š $$ \\ell(\\textbf{X}, \\textbf{y}, \\textbf{w}, b) = \\frac 1 {2n} \\sum_{i=1}^{2n} (y_i - \\lang{\\mathbf{x_i}, \\textbf{w}}\\rang - b)^2 = \\frac 1 {2n} \\lVert{\\textbf{y} - \\textbf{Xw} - b}\\rVert $$\nä»¤\\(\\textbf{X} = [\\textbf{X}, 1], \\textbf{w} = \\begin{bmatrix}\\textbf{w}\\\\b\\end{bmatrix}\\)ï¼Œåˆ™\\(\\ell(\\textbf{X}, \\textbf{y}, \\textbf{w}) = \\frac 1 {2n} \\lVert{\\textbf{y} - \\textbf{Xw}}\\rVert^2\\).\nç”±äºä¸Šå¼ä¸ºå‡¹å‡½æ•°ï¼Œæ•…å¯¹wæ±‚å¯¼ï¼Œå¯å–å‡ºæŸå¤±çš„æœ€å°å€¼ï¼Œæ­¤æ—¶çš„wï¼ˆå³åŸæ¥çš„wå’Œbï¼‰å–æœ€å°å€¼ã€‚ $$ \\frac \\partial {\\partial\\textbf{w}} \\ell(\\textbf{X}, \\textbf{y}, \\textbf{w}) = 0\\\\\\\\ \\Leftrightarrow \\frac 1 n (\\textbf{y} - \\textbf{Xw})^T\\textbf{X} = 0\\\\\\\\ \\Leftrightarrow \\mathbf{w}^* = (\\mathbf{X}^T\\textbf{X})^{-1}\\mathbf{X}^T\\textbf{y} $$\nçº¿æ€§å›å½’éƒ½æœ‰æ˜¾ç¤ºè§£ï¼Œä½†å¤§å¤šæ•°æ·±åº¦å­¦ä¹ æ¨¡å‹éƒ½æ²¡æœ‰æ˜¾ç¤ºè§£ï¼Œå› æ­¤ä¸èƒ½é€šè¿‡ç®€å•æ±‚å¯¼å¾—å‡ºç»“è®ºï¼Œä¸€èˆ¬å¯ä»¥é€šè¿‡ä¸€äº›ä¼˜åŒ–ç®—æ³•æ¥æ±‚è§£ï¼Œå¦‚ä¸‹æ‰€ç¤ºä¸ºåŸºç¡€ä¼˜åŒ–æ–¹æ³•ï¼ˆæ¢¯åº¦ä¸‹é™æ³•ï¼‰ã€‚\nä¸‹é¢çš„ä¾‹å­ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡ç»™å®šéšæœºçš„ä¸€ç»„äºŒç»´å‘é‡Xã€å…¬å¼\\(y = \\textbf{Xw} + b + \\epsilon\\)ï¼ˆ\\(\\epsilon\\)ä¸ºéšæœºå™ªå£°å‚æ•°ï¼Œæœä»æ ‡å‡†æ­£æ€åˆ†å¸ƒï¼‰å’Œyå€¼ï¼Œæ¥é¢„æµ‹çº¿æ€§å›å½’çš„å‚æ•°wå’Œbï¼Œé‡‡ç”¨æ¢¯åº¦ä¸‹é™çš„ä¼˜åŒ–æ–¹æ³•ã€‚\n%matplotlib inline # åµŒå…¥matplotlib import random import torch from d2l import torch as d2l def synthetic_data(w, b, num_examples): # ç”Ÿæˆy = Xw + b + å™ªå£° X = torch.normal(0, 1, (num_examples, len(w))) # normalä¸ºæ­£æ€åˆ†å¸ƒå‡½æ•°ï¼Œæ­¤å¤„æ„å»ºnum_examplesä¸ªç»´åº¦ä¸ºwé•¿åº¦çš„æœä»æ ‡å‡†æ­£æ€åˆ†å¸ƒçš„Xå€¼ y = torch.matmul(X, w) + b # matmulå°±æ˜¯çŸ©é˜µä¹˜æ³• y += torch.normal(0, 0.01, y.shape) return X, y.reshape(-1, 1) true_w = torch.tensor([2, -3.4]) true_b = 4.2 features, labels = synthetic_data(true_w, true_b, 1000) print(\u0026#39;features: \u0026#39;, features[0], \u0026#39;\\nlabel: \u0026#39;, labels[0]) features: tensor([0.8040, 1.4655]) label: tensor([0.8383])\rd2l.set_figsize() d2l.plt.scatter(features[:, 1].detach().numpy(), labels.detach().numpy(), 1) \u0026lt;matplotlib.collections.PathCollection at 0x25f0d8d5450\u0026gt;\rdef data_iter(batch_size, features, labels): num_examples = len(features) indices = list(range(num_examples)) # ç”Ÿæˆlist random.shuffle(indices) # æ‰“ä¹±ä¸‹æ ‡ for i in range(0, num_examples, batch_size): batch_indices = torch.tensor(indices[i:min(i + batch_size, num_examples)]) yield features[batch_indices], labels[batch_indices] # yieldç›¸å½“äºreturnï¼Œå±äºæƒ°æ€§è®¡ç®— batch_size = 10 for X, y in data_iter(batch_size, features, labels): print(X, \u0026#39;\\n\u0026#39;, y) break tensor([[ 0.5823, 1.3700],\r[-0.0490, -1.3720],\r[-0.7192, 0.1148],\r[-0.0823, -0.0522],\r[ 3.1452, -0.4425],\r[ 1.5709, 0.9346],\r[-1.4497, 1.3512],\r[-0.6982, -1.0002],\r[-0.3893, -0.1003],\r[ 1.8286, -0.7445]]) tensor([[ 0.7096],\r[ 8.7721],\r[ 2.3609],\r[ 4.2116],\r[11.9909],\r[ 4.1835],\r[-3.2944],\r[ 6.1959],\r[ 3.7565],\r[10.3933]])\r# ä¸ºwå’Œbè¿›è¡Œéšæœºåˆå§‹åŒ– w = torch.normal(0, 0.01, size=(2, 1), requires_grad=True) # å› ä¸ºXçš„sizeä¸º[n, 2]ï¼Œæ‰€ä»¥wçš„sizeä¸º[2, 1] b = torch.zeros(1, requires_grad=True) def linreg(X, w, b): return torch.matmul(X, w) + b çº¿æ€§å›å½’å¸¸ä½¿ç”¨å‡æ–¹æŸå¤±å‡½æ•°æ¥è®¡ç®—é¢„æµ‹å€¼ä¸çœŸå®å€¼çš„åå·®ï¼Œå‡æ–¹æŸå¤±ï¼ˆL2 Lossï¼‰å‡½æ•°çš„å…¬å¼ä¸ºï¼š $$ l(y, y^{\\prime}) = \\frac 1 2 (y - y^{\\prime})^2 $$\nä¹‹æ‰€ä»¥æœ‰ä¸€ä¸ª\\(\\frac 1 2\\)ï¼Œæ˜¯ä¸ºäº†æ–¹ä¾¿æ±‚å¯¼ã€‚\ndef squared_loss(y_hat, y): # å‡æ–¹æŸå¤± return (y_hat - y.reshape(y_hat.shape))**2 / 2 def sgd(params, lr, batch_size): # å°æ‰¹é‡éšæœºæ¢¯åº¦ä¸‹é™ with torch.no_grad(): # no_gradå‡½æ•°è¡¨ç¤ºæ¥ä¸‹æ¥ä¸è¦è®°å½•è®¡ç®—çš„æ¢¯åº¦ for param in params: param -= lr * param.grad / batch_size param.grad.zero_() lr = 0.03 num_epochs = 3 net = linreg loss = squared_loss for epoch in range(num_epochs): for X, y in data_iter(batch_size, features, labels): l = loss(net(X, w, b), y) # è®¡ç®—é¢„æµ‹çš„yå’ŒçœŸå®çš„yçš„å‡æ–¹æŸå¤± l.sum().backward() # å› ä¸ºlæ˜¯ä¸€ä¸ª[batch_size, 1]å¤§å°çš„çŸ¢é‡ï¼Œæ•…å¯¹å…¶æ±‚å’Œä»¥è®¡ç®—æ¢¯åº¦ sgd([w, b], lr, batch_size) # ä½¿ç”¨sgdå‡½æ•°è¿›è¡Œæ¢¯åº¦ä¸‹é™ with torch.no_grad(): train_l = loss(net(features, w, b), labels) print(f\u0026#39;epoch {epoch + 1}, loss {float(train_l.mean()): f}\u0026#39;) epoch 1, loss 0.040788\repoch 2, loss 0.000154\repoch 3, loss 0.000051\rprint(f\u0026#39;wçš„ä¼°è®¡è¯¯å·®ï¼š{true_w - w.reshape(true_w.shape)}\u0026#39;) print(f\u0026#39;bçš„ä¼°è®¡è¯¯å·®ï¼š{true_b - b}\u0026#39;) wçš„ä¼°è®¡è¯¯å·®ï¼štensor([5.7340e-05, 1.0276e-04], grad_fn=\u0026lt;SubBackward0\u0026gt;)\rbçš„ä¼°è®¡è¯¯å·®ï¼štensor([-0.0003], grad_fn=\u0026lt;RsubBackward1\u0026gt;)\rç”±äºå¾ˆå¤šå‡½æ•°å·²ç»è¢«å°è£…åœ¨Pytorchç­‰åº“ä¸­ï¼Œæ•…æ¯æ¬¡ä¸éœ€è¦å¦‚ä¸Šå¤æ‚èˆ¬è¿›è¡Œï¼Œä¸Šè¿°ä»£ç å¯ç®€åŒ–å¦‚ä¸‹ï¼š\nimport numpy as np import torch from torch.utils import data from d2l import torch as d2l true_w = torch.tensor([2, -3.4]) true_b = 4.2 features, labels = d2l.synthetic_data(true_w, true_b, 1000) def load_array(data_arrays, batch_size, is_train = True): # æ„é€ ä¸€ä¸ªPytorchçš„æ•°æ®è¿­ä»£å™¨ dataset = data.TensorDataset(*data_arrays) return data.DataLoader(dataset, batch_size, shuffle=is_train) batch_size = 10 data_iter = load_array((features, labels), batch_size) next(iter(data_iter)) [tensor([[-0.4498, -0.7477],\r[ 0.6690, 2.2404],\r[ 0.1602, 1.2009],\r[ 0.8746, 1.9034],\r[ 1.0719, 1.7307],\r[-1.4780, -2.2426],\r[ 0.7266, -1.5096],\r[-1.1183, -0.2499],\r[-0.9855, -0.6008],\r[ 0.9017, 0.9173]]),\rtensor([[ 5.8516],\r[-2.0883],\r[ 0.4374],\r[-0.5186],\r[ 0.4520],\r[ 8.8814],\r[10.7827],\r[ 2.8048],\r[ 4.2726],\r[ 2.8904]])]\rfrom torch import nn net = nn.Sequential(nn.Linear(2, 1)) net[0].weight.data.normal_(0, 0.01) net[0].bias.data.fill_(0) tensor([0.])\rloss = nn.MSELoss() # å®šä¹‰å‡æ–¹è¯¯å·®å®ç±» trainer = torch.optim.SGD(net.parameters(), lr = 0.03) # å®ä¾‹åŒ–SGDå®ä¾‹ num_epochs = 3 for epoch in range(num_epochs): for X, y in data_iter: l = loss(net(X), y) trainer.zero_grad() l.backward() trainer.step() l = loss(net(features), labels) print(f\u0026#39;epoch {epoch + 1}, loss {l:f}\u0026#39;) epoch 1, loss 0.000101\repoch 2, loss 0.000102\repoch 3, loss 0.000101\r3.2 Softmaxå›å½’ #åˆ†ç±»å’Œå›å½’çš„åŒºåˆ«ï¼š Softmaxå›å½’æœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªå¤šç±»åˆ†ç±»æ¨¡å‹ï¼Œå…¶è®¡ç®—å…¬å¼å¦‚ä¸‹ï¼š $$ \\hat{y_i} = \\frac {exp(o_i)} {\\sum_{k} exp(o_k)} $$\nSoftmaxå‡½æ•°å¸¸ä¸äº¤å‰ç†µæŸå¤±è”ç”¨ï¼Œåè€…ç”¨æ¥è¡¡é‡ä¸¤ä¸ªæ¦‚ç‡ä¹‹é—´çš„åŒºåˆ«ï¼Œå¸¸ç”¨äºäºŒåˆ†ç±»æˆ–å¤šåˆ†ç±»é—®é¢˜ã€‚äº¤å‰ç†µæŸå¤±å‡½æ•°ä¸ºï¼š $$ l(y, \\hat{y}) = - \\sum_{i} y_i log\\hat{y_i} = -log\\hat{y_y} $$ æ•…å…¶æ¢¯åº¦åˆšå¥½æ˜¯Softmaxé¢„æµ‹å€¼ä¸çœŸå®å€¼çš„å·®å€¼ï¼š $$ \\partial_{o_i}l(y, \\hat{y}) = softmax(o)_i - y_i $$ ä»è€Œå¯ä»¥é€šè¿‡æ¢¯åº¦ä¸‹é™æ³•è¿›è¡Œä¼˜åŒ–ã€‚\né™¤äº†äº¤å‰ç†µæŸå¤±å‡½æ•°å’Œå‡æ–¹æŸå¤±å‡½æ•°ï¼ŒL1 Lossä¹Ÿæ˜¯ä¸€ç»å…¸çš„æŸå¤±å‡½æ•°ï¼š $$ l(y, y^{\\prime}) = |y - y^{\\prime}| $$\nL2å‡½æ•°é€‚åˆäºå›å½’é—®é¢˜ï¼Œä½†æ˜¯å½“é¢„æµ‹å€¼äºçœŸå®å€¼è¾ƒè¿œæ—¶å¯¼æ•°å˜åŒ–å¤ªå¤§ã€‚L1å‡½æ•°è™½ç„¶æ²¡æœ‰è¿™ä¸ªé—®é¢˜ï¼Œä½†æ˜¯åœ¨åŸç‚¹å¤„ä¸å¯å¯¼ï¼Œå› æ­¤å½“é¢„æµ‹å€¼ä¸çœŸå®å€¼æ¥è¿‘æ—¶ï¼Œå˜åŒ–ä¸å¤ªç¨³å®šã€‚ç»“åˆä»¥ä¸ŠäºŒè€…çš„ä¼˜åŠ¿å¯ä»¥æå‡ºä¸€ä¸ªæ–°çš„æŸå¤±å‡½æ•°ï¼Œå¦‚Huber\u0026rsquo;s Robust Losså‡½æ•°ã€‚ $$ l(y, y^{\\prime}) = \\begin{cases} |y - y^{\\prime}| - \\frac 1 2 \u0026amp;\\text{if } |y - y^{\\prime}| \u0026gt; 1 \\\\\\\\ \\frac 1 2 (y - y^{\\prime})^2 \u0026amp;\\text{otherwise} \\end{cases} $$ ä¸‹é¢çš„ä»£ç ä¸ºä½¿ç”¨Pytorchæ¡†æ¶è¯»å…¥æ•°æ®é›†æ•°æ®çš„ç¤ºä¾‹ã€‚\n%matplotlib inline import torch import torchvision from torch.utils import data from torchvision import transforms from d2l import torch as d2l d2l.use_svg_display() # ä½¿ç”¨svgæ˜¾ç¤ºå›¾ç‰‡ä»¥è·å¾—æ›´é«˜æ¸…æ™°åº¦ trans = transforms.ToTensor() mnist_train = torchvision.datasets.FashionMNIST(root=\u0026#39;\u0026#39;, train=True, transform=trans, download=True) mnist_test = torchvision.datasets.FashionMNIST(root=\u0026#39;\u0026#39;, train=False, transform=trans, download=False) len(mnist_train), len(mnist_test) (60000, 10000)\rmnist_train[0][0].shape torch.Size([1, 28, 28])\rdef get_fashion_mnist_labels(labels): text_labels = [\u0026#39;t-shirt\u0026#39;, \u0026#39;trousers\u0026#39;, \u0026#39;pullover\u0026#39;, \u0026#39;dress\u0026#39;, \u0026#39;coat\u0026#39;, \u0026#39;sandal\u0026#39;, \u0026#39;shirt\u0026#39;, \u0026#39;sneaker\u0026#39;, \u0026#39;bag\u0026#39;, \u0026#39;ankle boot\u0026#39;] return [text_labels[int(i)] for i in labels] def show_images(imgs, num_rows, num_cols, titles=None, scale=1.5): figsize = (num_cols * scale, num_rows * scale) _, axes = d2l.plt.subplots(num_rows, num_cols, figsize=figsize) axes = axes.flatten() for i, (ax, img) in enumerate(zip(axes, imgs)): ax.set_title(titles[i]) ax.axis(\u0026#39;off\u0026#39;) if torch.is_tensor(img): ax.imshow(img.numpy()) else: ax.imshow(img) X, y = next(iter(data.DataLoader(mnist_train, batch_size=18))) show_images(X.reshape(18, 28, 28), 2, 9, titles=get_fashion_mnist_labels(y)) batch_size = 256 def get_dataloader_workers(): return 4 # ä½¿ç”¨4ä¸ªè¿›ç¨‹æ¥è¯»å–æ•°æ® train_iter = data.DataLoader(mnist_train, batch_size, shuffle=True, num_workers=get_dataloader_workers()) timer = d2l.Timer() # ä½¿ç”¨Timerå‡½æ•°è®¡ç®—æ•°æ®è¯»å…¥æ—¶é—´ for X, y in train_iter: continue f\u0026#39;{timer.stop():.2f} sec\u0026#39; '3.19 sec'\rä¸Šè¿°æ“ä½œå¯ä»¥å½’å¹¶è‡³ä¸€èµ·ï¼Œå¦‚ä¸‹æ‰€ç¤ºã€‚\ndef load_data_fashion_mnist(batch_size, resize=None): trans = [transforms.ToTensor()] if resize: trans.insert(0, transforms.Resize(resize)) trans = transforms.Compose(trans) mnist_train = torchvision.datasets.FashionMNIST(root=\u0026#39;\u0026#39;, train=True, transform=trans, download=True) mnist_test = torchvision.datasets.FashionMNIST(root=\u0026#39;\u0026#39;, train=False, transform=trans, download=False) return (data.DataLoader(mnist_train, batch_size, shuffle=True, num_workers=get_dataloader_workers()), data.DataLoader(mnist_test, batch_size, shuffle=False, num_workers=get_dataloader_workers())) ä¸‹é¢æ˜¯ä¸€æ®µä»0å¼€å§‹å®ç°çš„Softmaxå›å½’çš„ä»£ç ï¼Œç”±äºè¿™ç§ä»£ç è¿‡äºå¤æ‚ï¼Œæ•…åç»­ç« èŠ‚ä¸­æˆ‘ä¸»è¦å…³æ³¨å‡½æ•°çš„ç®€æ´å®ç°ã€‚\nimport torch from IPython import display from d2l import torch as d2l batch_size = 256 train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size) num_inputs = 784 num_outputs = 10 W = torch.normal(0, 0.01, size=(num_inputs, num_outputs), requires_grad=True) b = torch.zeros(num_outputs, requires_grad=True) X = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]]) X.sum(0, keepdim=True), X.sum(1, keepdim=True) (tensor([[5., 7., 9.]]),\rtensor([[ 6.],\r[15.]]))\rdef softmax(X): X_exp = torch.exp(X) partition = X_exp.sum(1, keepdim=True) return X_exp / partition # æ­¤å¤„åº”ç”¨äº†å¹¿æ’­æœºåˆ¶ X = torch.normal(0, 1, (2, 5)) X_prob = softmax(X) X_prob, X_prob.sum(1) (tensor([[0.5223, 0.0729, 0.0685, 0.2180, 0.1183],\r[0.6513, 0.1024, 0.0639, 0.1169, 0.0655]]),\rtensor([1., 1.]))\rdef net(X): # ä½¿ç”¨X*W+bè¿›è¡Œæ‹Ÿåˆ return softmax(torch.matmul(X.reshape((-1, W.shape[0])), W) + b) y = torch.tensor([0, 2]) y_hat = torch.tensor([[0.1, 0.3, 0.6], [0.3, 0.2, 0.5]]) y_hat[[0, 1], y] tensor([0.1000, 0.5000])\rdef cross_entropy(y_hat, y): # äº¤å‰ç†µæŸå¤±å‡½æ•° return -torch.log(y_hat[range(len(y_hat)), y]) cross_entropy(y_hat, y) tensor([2.3026, 0.6931])\rdef accuracy(y_hat, y): # è®¡ç®—é¢„æµ‹æ­£ç¡®çš„æ•°é‡ if len(y_hat.shape) \u0026gt; 1 and y_hat.shape[1] \u0026gt; 1: y_hat = y_hat.argmax(axis=1) cmp = y_hat.type(y.dtype) == y return float(cmp.type(y.dtype).sum()) accuracy(y_hat, y) / len(y) 0.5\rdef evaluate_accuracy(net, data_iter): if isinstance(net, torch.nn.Module): # isinstanceå‡½æ•°æ˜¯ç”¨æ¥åˆ¤æ–­ä¸€ä¸ªå¯¹è±¡æ˜¯å¦æ˜¯æŒ‡å®šç±»çš„å®ä¾‹ï¼Œæ­¤å¤„åˆ¤æ–­netæ˜¯å¦ä¸ºtorch.nn.Moduleçš„å®ä¾‹ï¼Œå³netæ˜¯å¦ä¸ºä¸€ä¸ªç¥ç»ç½‘ç»œæ¨¡å‹ net.eval() # å°†æ¨¡å‹è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼ metric = Accumulator(2) for X, y in data_iter: metric.add(accuracy(net(X), y), y.numel()) return metric[0] / metric[1] class Accumulator: def __init__(self, n): self.data = [0, 0] * n def add(self, *args): self.data = [a + float(b) for a, b in zip(self.data, args)] def reset(self): self.data = [0.0] * len(self.data) def __getitem__(self, idx): return self.data[idx] evaluate_accuracy(net, test_iter) 0.1208\rdef train_epoch_ch3(net, train_iter, loss, updater): if isinstance(net, torch.nn.Module): net.train() metric = Accumulator(3) for X, y in train_iter: y_hat = net(X) l = loss(y_hat, y) if isinstance(updater, torch.optim.Optimizer): updater.zero_grad() l.backward() updater.step() metric.add(float(l) * len(y), accuracy(y_hat, y), y.size().numel()) else: l.sum().backward() updater(X.shape[0]) metric.add(float(l.sum()), accuracy(y_hat, y), y.numel()) return metric[0] / metric[2], metric[1] / metric[2] class Animator: def __init__(self, xlabel=None, ylabel=None, legend=None, xlim=None, ylim=None, xscale=\u0026#39;linear\u0026#39;, yscale=\u0026#39;linear\u0026#39;, fmts=(\u0026#39;-\u0026#39;, \u0026#39;m--\u0026#39;, \u0026#39;g-.\u0026#39;, \u0026#39;r:\u0026#39;), nrows=1, ncols=1, figsize=(3.5, 2.5)): if legend is None: legend = [] d2l.use_svg_display() self.fig, self.axes = d2l.plt.subplots(nrows, ncols, figsize=figsize) if nrows * ncols == 1: self.axes = [self.axes, ] self.config_axes = lambda: d2l.set_axes(self.axes[0], xlabel, ylabel, xlim, ylim, xscale, yscale, legend) self.X, self.Y, self.fmts = None, None, fmts def add(self, x, y): if not hasattr(y, \u0026#34;__len__\u0026#34;): y = [y] n = len(y) if not hasattr(x, \u0026#34;__len__\u0026#34;): x = [x] * n if not self.X: self.X = [[] for _ in range(n)] if not self.Y: self.Y = [[] for _ in range(n)] for i, (a, b) in enumerate(zip(x, y)): if a is not None and b is not None: self.X[i].append(a) self.Y[i].append(b) self.axes[0].cla() for x, y, fmt in zip(self.X, self.Y, self.fmts): self.axes[0].plot(x, y, fmt) self.config_axes() display.display(self.fig) display.clear_output(wait=True) def train_ch3(net, train_iter, test_iter, loss, num_epochs, updater): animator = Animator(xlabel=\u0026#39;epoch\u0026#39;, xlim=[1, num_epochs], ylim=[0.3, 0.9], legend=[\u0026#39;train loss\u0026#39;, \u0026#39;train acc\u0026#39;, \u0026#39;test acc\u0026#39;]) for epoch in range(num_epochs): train_metrics = train_epoch_ch3(net, train_iter, loss, updater) test_acc = evaluate_accuracy(net, test_iter) animator.add(epoch + 1, train_metrics + (test_acc, )) train_loss, train_acc = train_metrics lr = 0.1 def updater(batch_size): return d2l.sgd([W, b], lr, batch_size) num_epochs = 10 train_ch3(net, train_iter, test_iter, cross_entropy, num_epochs, updater) def predict_ch3(net, test_iter, n=6): for X, y in test_iter: break trues = d2l.get_fashion_mnist_labels(y) preds = d2l.get_fashion_mnist_labels(net(X).argmax(axis=1)) titles = [true + \u0026#39;\\n\u0026#39; + pred for true, pred in zip(trues, preds)] d2l.show_images(X[0:n].reshape(n, 28, 28), 1, n, titles=titles[0:n]) predict_ch3(net, test_iter) Softmaxå›å½’ç²¾ç®€ç‰ˆï¼š\nimport torch from torch import nn from d2l import torch as d2l batch_size = 256 train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size) net = nn.Sequential(nn.Flatten(), nn.Linear(784, 10)) def init_weights(m): if type(m) == nn.Linear: nn.init.normal_(m.weight, std=0.01) # stdä¸ºæ ‡å‡†å·®å‚æ•° net.apply(init_weights) Sequential(\r(0): Flatten(start_dim=1, end_dim=-1)\r(1): Linear(in_features=784, out_features=10, bias=True)\r)\rloss = nn.CrossEntropyLoss() trainer = torch.optim.SGD(net.parameters(), lr=0.1) num_epochs = 10 d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer) ","date":"2024å¹´7æœˆ26æ—¥","permalink":"/zh-cn/study/pytorch/","section":"Studies","summary":"æœ€è¿‘åœ¨è·Ÿç€ææ²å¤§ç¥å­¦ä¹ Pytorchï¼Œä»¥æ­¤è®°å½•ä¸‹æˆ‘æ‰€å­¦ä¹ çš„ä¸€äº›çŸ¥è¯†ã€‚æ–°æ‰‹ä¸Šè·¯ï¼Œè¿˜æœ›å¤šå¤šæŒ‡æ•™ã€‚","title":"Pytorchå­¦ä¹ ç¬”è®°"},{"content":"","date":null,"permalink":"/zh-cn/study/","section":"Studies","summary":"","title":"Studies"},{"content":"è€ƒç ”ç»“æŸåï¼Œæˆ‘çš„å†…å¿ƒæŠ±æœ‰æŒä¹…çš„ä¼¤ç—›ï¼Œé‚£äº›ä¼¤ç—›ä¸€ç›´è¦ç»•æˆ‘çš„å¿ƒå¤´ï¼Œç›´è‡³å¤§å­¦æ¯•ä¸šï¼Œè°¨ä»¥æ­¤æ–‡çºªå¿µæˆ‘é‚£æœ€åçš„å¤§å­¦æ—¶å…‰ã€‚\nè¥¿å®‰ #å»è¥¿å®‰æ˜¯å»é¢è¯•èˆªç©ºæ‰€ï¼ˆ631æ‰€ï¼‰çš„è°ƒå‰‚ã€‚å½“åˆæŠ¥åä¹Ÿä»…æ˜¯å› ä¸ºå®ƒæ˜¯ä¸ºæ•°ä¸å¤šæ‹›æ”¶è®¡ç®—æœºç§‘å­¦æŠ€æœ¯ï¼ˆå­¦ç¡•ï¼‰çš„è°ƒå‰‚å•ä½ï¼Œè€Œä¸”æŠ¥é”€å¾€è¿”äº¤é€šã€ä½å®¿è´¹ç”¨ã€‚ç”±æ­¤è¸ä¸Šå‰å¾€è¥¿å®‰çš„æ—…é€”ã€‚\nç®€å•åœ°è¯´ï¼Œé¢è¯•é»„äº†ï¼Œä½†æ‰€å¹¸åœ¨é¢è¯•ç»“æŸåå½“æ™šå°±æ”¶åˆ°äº†æ¸¯ç§‘å¹¿çš„é¢è¯•é‚€è¯·ï¼Œä¹Ÿç®—ç»™äº†æˆ‘æœ€åçš„å¸Œæœ›ã€‚\nä¸´èµ°çš„é‚£å¤©ï¼Œæˆ‘ç‰¹æ„è®©å¸æœºç»•äº†ä¸‹é™•è¥¿å†å²åšç‰©é¦†å’Œå¤§é›å¡”ã€‚çœ‹åˆ°å¤§é›å¡”çš„ç¬é—´ï¼Œæˆ‘ä¸ç¦æƒ³èµ·äº†åå¤šå¹´å‰çš„é‚£ä¸ªä¸‹æ™šï¼Œæˆ‘éšçˆ¶æ¯æ¥è¥¿å®‰æ—…æ¸¸ï¼Œä»æ˜¯åœ¨æ—…æ¸¸å¤§å·´è½¦ä¸Šé¥æœ›å¤§é›å¡”ã€‚å½¼æ—¶çš„æˆ‘æˆ–è®¸æ°¸è¿œä¹Ÿæƒ³ä¸åˆ°æˆ‘ä¼šä»¥æ­¤æ–¹å¼å†ä¸´è¥¿å®‰å§ã€‚å´ä¸æƒ³å› æ­¤é”™è¿‡äº†èˆªç­ï¼Œåªå¥½åœ¨å’¸é˜³æœºåœºæ—å¤šä½äº†ä¸€æ™šï¼Œä¸ºæˆ‘è¿™æœ¬å°±è’å”çš„å‡ºè¡Œæ›´æ·»ä¸€ä¸å¥‡å¼‚ã€‚\nç¬¬äºŒå¤©ä¸€æ—©ï¼Œå¸¦ç€ä¸€ä¸ç–²æƒ«ï¼Œæˆ‘ç»ˆäºç¦»å¼€äº†è¿™ä¸ªåƒå¹´å¤éƒ½ã€‚\nå¹¿å· \u0026amp; ä½›å±± #å¾ˆå¤šæ—¶å€™ï¼Œå¾ˆå¤šäººï¼Œå¾ˆå¤šäº‹éƒ½æ˜¯åœ¨æœºç¼˜å·§åˆä¸‹å‘ç”Ÿçš„ï¼Œè€Œæ›´å¤šçš„æ—¶å€™ï¼Œäººéƒ½åƒä¸€åªæ— å¤´è‹è‡èˆ¬åœ¨è‡ªå·±çš„äººç”Ÿè½¨è¿¹ä¸Šä¹±æ’ï¼Œä»¥å›¾æ’å‡ºé‚£å°˜å°å¤šå¹´çš„æ·é”ã€‚æˆ‘æ¥å¹¿å·æ­£æ˜¯å¦‚æ­¤ï¼Œä¹Ÿæ­£å¦‚æˆ‘æŠ¥åæ¸¯ç§‘å¹¿ä¸åŒ—å¤§è½¯å¾®ä¸€æ ·ï¼Œä¸è¿‡éƒ½æ˜¯åœ¨è·Ÿé£ä¸­å¦„å›¾é€†å¤©æ”¹å‘½ç½¢äº†ã€‚åªæ˜¯äº‹ç‰©å¤šæœ‰è”ç³»æ€§ï¼Œç¯ç¯ç›¸æ‰£ï¼Œæœ€ç»ˆè¿˜æ˜¯é€ƒä¸å‡ºè¿™ä¸€ç¯åˆä¸€ç¯ã€‚\näº”æœˆæœ«çš„å¹¿å·å·²ç„¶ååˆ†ç‚çƒ­ï¼Œä¼´éšç€æµ·æ´‹æ€§æ°”å€™ç‰¹æœ‰çš„æ¹¿æ¶¦ï¼Œå¾ˆéš¾è®©äººåœ¨å‡ºè¡Œåä¸å‡èµ·ä¸€èº«æ±—ç ã€‚å³ä½¿åœ¨å†·æ°”å……è¶³çš„ç©ºè°ƒæˆ¿é‡Œï¼Œè±†å¤§çš„æ±—ç ä»æŒ‚æ»¡æˆ‘çš„é¢ç¨ï¼Œä»¤åˆšé¢è¯•å®Œçš„æˆ‘ç•¥æ˜¾ç—…æ€ã€‚è™½æœªå¯çŸ¥ç»“æœå¦‚ä½•ï¼Œæˆ‘å·²çœ‹å¾—å¼€äº›äº†ã€‚æ¥ä¸‹æ¥çš„å‡ å¤©é‡Œï¼Œæˆ‘è™½æ— å¿ƒå‡ºæ¸¸ï¼Œä½†èººåœ¨é…’åº—é‡Œæ‰“æ¸¸æˆçš„å¿ƒè¿˜æ˜¯æœ‰çš„ã€‚ä¸å¥½å‹çº¦ä¸Šä¸¤å±€ä¹‹åï¼Œæ— å°½çš„ç©ºè™šå’Œææƒ§åˆè¢­æ»¡æˆ‘èº«ã€‚\næ¬¡æ—¥æ¸¸äº†ä½›å±±ï¼Œä¸»è¦çœ‹äº†çœ‹ç¥–åº™ï¼Œè§äº†è§é»„é£é¸¿çš„å¾’å­å¾’å­™ï¼Œç¨åƒäº†ç‚¹é¡ºå¾·ç‰¹äº§ï¼Œä¾¿å›å»äº†ã€‚è€å®è¯´ï¼Œè¿™äº›æ›¾ç»ä»¤æˆ‘ç—´è¿·å¤šå¹´çš„æ™¯ç‚¹æ­¤åˆ»ç«Ÿéš¾ä»¥æèµ·æˆ‘çš„ä¸€ç‚¹å…´è¶£ã€‚\nä¸´è¡Œå‰ï¼Œæˆ‘ç‰¹æ„å»å¹¿å·â€œå°è›®è…°â€å’Œæµ·å¿ƒæ¡¥çœ‹äº†çœ‹ã€‚æˆ‘åœ¨å¿ƒä¸­é»˜é»˜æƒ³ç€ï¼Œä»¥åèƒ½æ¥æ­¤å¤„ä¸Šå­¦ï¼Œæ—¥åæ¥ç©çš„æœºä¼šè¿˜å¤šç€å‘¢ï¼›å€˜è‹¥æ¥ä¸äº†ï¼Œè¿™éæˆ‘ä¹Ÿæ— æ‰€è°“äº†ã€‚\nä»ç™½äº‘æœºåœºç¦»å¼€æ—¶ï¼Œç™½äº‘è¢«ä¹Œäº‘é®ä½äº†å¤§éƒ¨ã€‚\næ¯•ä¸š #è¿™ä¸ªç« èŠ‚æˆ‘æœ¬ä¸æƒ³å†™çš„ï¼Œæ¯•ç«Ÿç ´åäº†æˆ‘ç« èŠ‚çš„æœ‰åºæ€§ï¼Œä½†æƒ³åˆ°è¿™æ¯•ç«Ÿä¹Ÿæ˜¯æˆ‘äººç”Ÿçš„ä¸€ä¸ªè½¬æŠ˜ç‚¹ï¼Œæ­¤åˆ»ä¸å†™ï¼Œæ—¥åä¹Ÿéš¾æœ‰å›å¿†äº†ï¼Œä¾¿åœä¸ä½ç¬”å¢¨ï¼Œç®€å•èŠèŠã€‚\nç®€å•è€Œè¨€ï¼Œè¿™æ—¶çš„æˆ‘å·²ç»æ”¶åˆ°äº†æ¸¯ç§‘å¹¿çš„å½•å–é€šçŸ¥ä¹¦ï¼Œå› è€Œä¹Ÿç•¥æ€€å¼€å¿ƒåœ°ä¸åŒå­¦ä»¬ä¸€èµ·åˆå½±ï¼Œç…§äº†å¾ˆå¤šæ¯•ä¸šç…§ã€‚ä½†æ˜¯è¯´æœ‰å¤šå¼€å¿ƒï¼Œæ˜¯ç»å¯¹æ²¡æœ‰çš„ã€‚\næˆ‘æ›¾æ— æ•°æ¬¡åœ¨ç¡è§‰å‰ç•…æƒ³è¿‡æ¯•ä¸šå…¸ç¤¼çš„åœºæ™¯ï¼Œå¤§çº¦ä»æˆ‘å†³å®šè€ƒç ”èµ·ï¼Œæˆ‘å°±çŸ¥é“æ¯•ä¸šå…¸ç¤¼è¦ä¹ˆç¬‘ç€è¿‡ï¼Œè¦ä¹ˆè‹¦ç¬‘ç€è¿‡ã€‚ä½†æ­¤åˆ»ä¸€æƒ³ï¼Œæˆ‘è„‘ä¸­ç»ˆç©¶è¿˜æ˜¯å“èµ·äº†é‚£é¦–ã€Šæˆ‘ç»ˆäºå¤±å»äº†ä½ ã€‹ã€‚æˆ‘æ¨æˆ‘å››å¹´çš„åŠªåŠ›åŒ–ä¸ºæ³¡å½±ï¼æˆ‘é—æ†¾æˆ‘è¿‡æ—©åœ°é€‰æ‹©äº†è€ƒç ”ï¼æˆ‘ä¼¤ç—›æˆ‘çš„å››å¹´å¦‚æ¢¦å¦‚å¹»ã€é£é€å¦‚ç”µï¼Œå´ä¸€äº‹æ— æˆï¼æ­£å¦‚æˆ‘å››å¹´å‰èµ°è¿›è¿™æ‰€å­¦æ ¡ä¸€æ ·ã€‚\nå—äº¬ \u0026amp; é•‡æ±Ÿ \u0026amp; æ‰¬å· #åœ¨å‰å¾€è¿™æ¡çº¿è·¯å‰ï¼Œæˆ‘æ›¾åšè¿‡ä¸‰æ¬¡è¯¦ç»†çš„è®¡åˆ’ï¼Œåˆ†åˆ«æ˜¯æ²³è¥¿èµ°å»Šã€é¡ºæ±Ÿè€Œä¸‹ã€åŒ—ä¸Šè¾½ä¸œï¼Œä½†æ˜¯éƒ½å› æ±ªå‡¯ç‘çš„å› ç´ è€½è¯¯äº†ï¼Œæœ€ç»ˆé€‰æ‹©äº†è¿™æœ€æœ´å®æ— åã€æœ€è¿‘ã€æœ€çŸ­çš„æ±Ÿè‹ä¹‹æ—…ã€‚ç°ä¸‹æƒ³æƒ³ï¼Œäººç”Ÿåˆä½•å°ä¸æ˜¯å¦‚æ­¤å‘¢ï¼Ÿ\næ­¤ä¸‰åœ°æˆ‘ä¸æƒ³å¤šåŠ èµ˜è¿°ï¼Œæ— éæ˜¯ç™½å¤©å‡ºæ¸¸ï¼Œå¤œæ™šæ¸¸æˆã€‚å°½ç®¡å‰æœŸå‡†å¤‡å¾ˆä¸çˆ½ï¼Œä½†æ˜¯ä¸€èµ·ç©ã€Šæ–‡æ˜6ã€‹çš„æ—¥å­è¿˜æ˜¯å€¼å¾—æ€€å¿µçš„ï¼\nä»¥ä¸Šä¸ºæˆ‘2024ä¸ŠåŠå¹´çš„å¹´åº¦æ€»ç»“ï¼Œä¹Ÿæ˜¯æˆ‘å¤§å­¦çš„æœ€ç»ˆç« ã€‚äººç”Ÿåœ¨ä¸–ï¼Œè‹¦å¤šä¹å°‘ã€‚é€†å¤©æ”¹å‘½ï¼Œç»ˆå½’å¹³å‡¡ã€‚å²‚çŸ¥äººç”Ÿä½•è‹¦è¿½å¿†å¾€æ˜”ï¼Œæˆ‘çš„ä¸€ä¸ªæ—¶ä»£ç»ˆç»“äº†ï¼Œæˆ‘å°†è¿™ä»½è®°å¿†å°˜å°äºæ­¤ï¼Œä¸å†æ‰“å¼€ã€‚\n","date":"2024å¹´7æœˆ26æ—¥","permalink":"/zh-cn/life/graduation_season/","section":"ç”Ÿæ´»","summary":"å¤œè‰²å¾®å‡‰ï¼Œä¸è§é‚£ä¸€æŠ¹æ®‹é˜³","title":"å¤§å­¦ Â· æœ€ç»ˆç« "},{"content":"1. Diffusion Model #","date":"2024å¹´7æœˆ26æ—¥","permalink":"/zh-cn/study/nn/","section":"Studies","summary":"ç†è®ºçŸ¥è¯†æ¥æºäºå‘¨å¿—åã€Šæœºå™¨å­¦ä¹ ã€‹ï¼Œæˆ‘å°†åœ¨æ­¤é¡µè®°å½•ä¸‹æˆ‘è®¤ä¸ºé‡è¦çš„çŸ¥è¯†ç‚¹ã€‚","title":"æœºå™¨å­¦ä¹ "},{"content":"","date":null,"permalink":"/zh-cn/categories/","section":"Categories","summary":"","title":"Categories"},{"content":"æˆ‘ç›®å‰æ˜¯ä¸€åé¦™æ¸¯ç§‘æŠ€å¤§å­¦ï¼ˆå¹¿å·ï¼‰çš„äººå·¥æ™ºèƒ½ç ”ç©¶å‹ç¡•å£«ç”Ÿã€‚åœ¨æ­¤ä¹‹å‰ï¼Œæˆ‘åœ¨æµ™æ±Ÿå¤§å­¦è·å¾—äº†å†œå­¦å­¦å£«å­¦ä½ã€‚\nåœ¨æœ¬ç§‘å­¦ä¹ æœŸé—´ï¼Œæˆ‘å‚ä¸äº†ä¸€äº›ä¸å†œä¸šæœºå™¨äººå’Œæ˜†è™«å›¾åƒè¯†åˆ«ç›¸å…³çš„é¡¹ç›®ï¼Œè¿™æ¿€å‘äº†æˆ‘å¯¹äººå·¥æ™ºèƒ½çš„å…´è¶£ã€‚åœ¨ä»£ç æ–¹é¢æˆ‘æ˜¯åŠè·¯å‡ºå®¶ï¼Œä»ç„¶æ˜¯ä¸ªèœé¸ŸğŸ¤¡ã€‚\næˆ‘å°†åœ¨æœ¬åšå®¢ä¸­ä¸å®šæœŸæ›´æ–°æˆ‘çš„ç”Ÿæ´»ğŸƒâ€â™‚ï¸ã€å­¦ä¹ ğŸ“–å’Œç§‘ç ”ğŸ–¥ï¸ç›¸å…³ã€‚å¸Œæœ›è®¿é—®æœ¬ç½‘ç«™çš„æœ‹å‹å¯ä»¥å¤šå¤šæŒ‡æ•™ï¼Œå°½æƒ…è®¨è®ºã€‚ğŸ‘‹ğŸ¤£\n","date":null,"permalink":"/zh-cn/about/","section":"å…³äº","summary":"","title":"å…³äº"},{"content":"\rè¿™æ˜¯æœ¬åšå®¢çš„ç§‘ç ”æ¿å—ï¼ğŸš€ æˆ‘å°†è®°å½•ä¸€äº›æˆ‘åœ¨é¦™æ¸¯ç§‘æŠ€å¤§å­¦ï¼ˆå¹¿å·ï¼‰æ”»è¯»ç¡•å£«æœŸé—´å°†è¿›è¡Œçš„ç ”ç©¶ã€‚ä½ å¯ä»¥åœ¨è¯„è®ºåŒºå°½æƒ…è¯„è®ºï¼Œæˆ–è€…å¦‚æœä½ ä¹Ÿæœ‰ä¸€äº›æœ‰è¶£çš„ç»å†ï¼Œæ¬¢è¿ä½ åœ¨è¯„è®ºåŒºåˆ†äº«ã€‚â­\ræ–°çš„å¼€å§‹ï¼Œæ–°çš„ç§‘ç ”ï¼ âœŠ #","date":null,"permalink":"/zh-cn/research/","section":"ç§‘ç ”","summary":"","title":"ç§‘ç ”"}]